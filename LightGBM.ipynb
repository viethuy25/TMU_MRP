{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415e9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe9d59",
   "metadata": {},
   "source": [
    "# LightGBM, for first run, uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8892f3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>encoded_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.428620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.906781e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.486586e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578240</td>\n",
       "      <td>0.999839</td>\n",
       "      <td>0.016088</td>\n",
       "      <td>6.101171e-03</td>\n",
       "      <td>3.595742e-01</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.037998</td>\n",
       "      <td>6.038352e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.081934e-03</td>\n",
       "      <td>0.039148</td>\n",
       "      <td>0.764455</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.019570</td>\n",
       "      <td>2.624302e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.467841</td>\n",
       "      <td>0.996626</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.092086e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.040388e-04</td>\n",
       "      <td>0.028460</td>\n",
       "      <td>0.859879</td>\n",
       "      <td>0.242375</td>\n",
       "      <td>0.361443</td>\n",
       "      <td>3.157241e-02</td>\n",
       "      <td>6.552889e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.322503</td>\n",
       "      <td>0.989857</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>1.015950e-06</td>\n",
       "      <td>7.954423e-01</td>\n",
       "      <td>0.412684</td>\n",
       "      <td>0.127934</td>\n",
       "      <td>0.015942</td>\n",
       "      <td>0.116336</td>\n",
       "      <td>6.150468e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.787286e-04</td>\n",
       "      <td>0.033901</td>\n",
       "      <td>0.751330</td>\n",
       "      <td>0.049434</td>\n",
       "      <td>0.032961</td>\n",
       "      <td>4.568948e-02</td>\n",
       "      <td>1.137700e-03</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388330</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388331</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388332</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388333</th>\n",
       "      <td>0.293524</td>\n",
       "      <td>0.951149</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.639506e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006017</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.380474e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.850077e-05</td>\n",
       "      <td>0.009008</td>\n",
       "      <td>0.996098</td>\n",
       "      <td>0.070973</td>\n",
       "      <td>0.048470</td>\n",
       "      <td>1.586959e-01</td>\n",
       "      <td>6.269141e-01</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388334</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5388335 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2     feature_3     feature_4  \\\n",
       "0         0.428620   1.000000   0.509812  0.000000e+00  8.906781e-07   \n",
       "1         0.578240   0.999839   0.016088  6.101171e-03  3.595742e-01   \n",
       "2         0.467841   0.996626   0.002890  0.000000e+00  0.000000e+00   \n",
       "3         0.322503   0.989857   0.023677  1.015950e-06  7.954423e-01   \n",
       "4         0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "...            ...        ...        ...           ...           ...   \n",
       "5388330   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "5388331   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "5388332   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "5388333   0.293524   0.951149   0.006017  0.000000e+00  4.639506e-07   \n",
       "5388334   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "\n",
       "         feature_5  feature_6  feature_7  feature_8     feature_9  ...  \\\n",
       "0         0.000000   0.000000   0.509813   0.000001  2.486586e-06  ...   \n",
       "1         0.000223   0.000125   0.016082   0.037998  6.038352e-05  ...   \n",
       "2         0.000000   0.000000   0.002891   0.000000  4.092086e-04  ...   \n",
       "3         0.412684   0.127934   0.015942   0.116336  6.150468e-05  ...   \n",
       "4         0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "...            ...        ...        ...        ...           ...  ...   \n",
       "5388330   0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "5388331   0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "5388332   0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "5388333   0.000000   0.000000   0.006017   0.000001  2.380474e-04  ...   \n",
       "5388334   0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "\n",
       "         feature_11    feature_12  feature_13  feature_14  feature_15  \\\n",
       "0               0.0  9.179891e-05    0.022490    0.680028    1.000000   \n",
       "1               1.0  5.081934e-03    0.039148    0.764455    0.000006   \n",
       "2               0.0  1.040388e-04    0.028460    0.859879    0.242375   \n",
       "3               0.0  2.787286e-04    0.033901    0.751330    0.049434   \n",
       "4               0.0  1.587743e-08    0.000004    0.580702    0.000543   \n",
       "...             ...           ...         ...         ...         ...   \n",
       "5388330         0.0  1.587743e-08    0.000004    0.580702    0.000543   \n",
       "5388331         0.0  1.587743e-08    0.000004    0.580702    0.000543   \n",
       "5388332         0.0  1.587743e-08    0.000004    0.580702    0.000543   \n",
       "5388333         0.0  8.850077e-05    0.009008    0.996098    0.070973   \n",
       "5388334         0.0  1.587743e-08    0.000004    0.580702    0.000543   \n",
       "\n",
       "         feature_16    feature_17    feature_18  feature_19  encoded_tags  \n",
       "0          0.459038  6.565172e-07  8.784210e-09    0.000003           184  \n",
       "1          0.019570  2.624302e-01  0.000000e+00    0.000404           141  \n",
       "2          0.361443  3.157241e-02  6.552889e-06    0.000009           194  \n",
       "3          0.032961  4.568948e-02  1.137700e-03    0.000133            60  \n",
       "4          1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "...             ...           ...           ...         ...           ...  \n",
       "5388330    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "5388331    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "5388332    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "5388333    0.048470  1.586959e-01  6.269141e-01    0.000092           142  \n",
       "5388334    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "\n",
       "[5388335 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed_data_expanded = pd.read_csv('./output/scaled_train_data.csv')\n",
    "# train_processed_data_expanded = train_processed_data_expanded.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "train_processed_data_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06388b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>encoded_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566236</td>\n",
       "      <td>0.697855</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.972780e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>2.248993e-02</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395012</td>\n",
       "      <td>0.999643</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.524506e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>1.076902e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.002599e-05</td>\n",
       "      <td>6.486638e-03</td>\n",
       "      <td>0.730756</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>6.395042e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387087</td>\n",
       "      <td>0.560943</td>\n",
       "      <td>0.115950</td>\n",
       "      <td>3.422385e-09</td>\n",
       "      <td>7.597835e-01</td>\n",
       "      <td>0.928852</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.115950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.698501e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.437215e-09</td>\n",
       "      <td>1.528109e-08</td>\n",
       "      <td>0.184108</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.197636</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.319294e-05</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.481720</td>\n",
       "      <td>0.478761</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>4.569686e-06</td>\n",
       "      <td>1.391312e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>5.133540e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.723015e-06</td>\n",
       "      <td>3.776245e-05</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>0.017997</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>2.773921e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123244</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123245</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123246</th>\n",
       "      <td>0.428620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.906781e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.486586e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>2.248993e-02</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123247</th>\n",
       "      <td>0.288678</td>\n",
       "      <td>0.987561</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>2.722036e-03</td>\n",
       "      <td>1.401579e-02</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>4.493264e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.863601e-04</td>\n",
       "      <td>2.160720e-02</td>\n",
       "      <td>0.905640</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.066771</td>\n",
       "      <td>1.111779e-01</td>\n",
       "      <td>2.173034e-02</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123248</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1123249 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2     feature_3     feature_4  \\\n",
       "0         0.566236   0.697855   0.009106  0.000000e+00  0.000000e+00   \n",
       "1         0.395012   0.999643   0.012381  0.000000e+00  9.524506e-04   \n",
       "2         0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "3         0.387087   0.560943   0.115950  3.422385e-09  7.597835e-01   \n",
       "4         0.481720   0.478761   0.001350  4.569686e-06  1.391312e-02   \n",
       "...            ...        ...        ...           ...           ...   \n",
       "1123244   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "1123245   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "1123246   0.428620   1.000000   0.509812  0.000000e+00  8.906781e-07   \n",
       "1123247   0.288678   0.987561   0.033791  2.722036e-03  1.401579e-02   \n",
       "1123248   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "\n",
       "         feature_5  feature_6  feature_7  feature_8     feature_9  ...  \\\n",
       "0         0.000000   0.000000   0.009106   0.000000  3.972780e-05  ...   \n",
       "1         0.000000   0.000000   0.012382   0.000697  1.076902e-04  ...   \n",
       "2         0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "3         0.928852   0.000010   0.115950   1.000000  3.698501e-06  ...   \n",
       "4         0.000000   0.000000   0.001350   0.003393  5.133540e-05  ...   \n",
       "...            ...        ...        ...        ...           ...  ...   \n",
       "1123244   0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "1123245   0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "1123246   0.000000   0.000000   0.509813   0.000001  2.486586e-06  ...   \n",
       "1123247   0.000009   0.000208   0.033779   0.001292  4.493264e-05  ...   \n",
       "1123248   0.000000   0.000000   1.000000   0.052762  7.937312e-07  ...   \n",
       "\n",
       "         feature_11    feature_12    feature_13  feature_14  feature_15  \\\n",
       "0               0.0  9.179891e-05  2.248993e-02    0.680028    1.000000   \n",
       "1               0.0  4.002599e-05  6.486638e-03    0.730756    0.007162   \n",
       "2               0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "3               1.0  3.437215e-09  1.528109e-08    0.184108    0.000010   \n",
       "4               1.0  1.723015e-06  3.776245e-05    0.094714    0.017997   \n",
       "...             ...           ...           ...         ...         ...   \n",
       "1123244         0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "1123245         0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "1123246         0.0  9.179891e-05  2.248993e-02    0.680028    1.000000   \n",
       "1123247         1.0  8.863601e-04  2.160720e-02    0.905640    0.000995   \n",
       "1123248         0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "\n",
       "         feature_16    feature_17    feature_18  feature_19  encoded_tags  \n",
       "0          0.459038  6.565172e-07  8.784210e-09    0.000003           184  \n",
       "1          0.013356  6.395042e-04  0.000000e+00    0.000405           180  \n",
       "2          1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "3          0.197636  1.000000e+00  1.319294e-05    0.000027           154  \n",
       "4          0.015916  2.773921e-01  0.000000e+00    0.000272            19  \n",
       "...             ...           ...           ...         ...           ...  \n",
       "1123244    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "1123245    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "1123246    0.459038  6.565172e-07  8.784210e-09    0.000003           184  \n",
       "1123247    0.066771  1.111779e-01  2.173034e-02    0.000083            75  \n",
       "1123248    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "\n",
       "[1123249 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_processed_data = pd.read_csv('./output/non_nan_balanced_data.csv')\n",
    "val_processed_data_expanded = pd.read_csv('./output/scaled_val_data.csv') \n",
    "# processed_data_expanded = processed_data_expanded.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "val_processed_data_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a46f540f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566236</td>\n",
       "      <td>0.697855</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.972780e-05</td>\n",
       "      <td>0.171641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>2.248993e-02</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395012</td>\n",
       "      <td>0.999643</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.524506e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>1.076902e-04</td>\n",
       "      <td>0.632633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.002599e-05</td>\n",
       "      <td>6.486638e-03</td>\n",
       "      <td>0.730756</td>\n",
       "      <td>0.007162</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>6.395042e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>0.376586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387087</td>\n",
       "      <td>0.560943</td>\n",
       "      <td>0.115950</td>\n",
       "      <td>3.422385e-09</td>\n",
       "      <td>7.597835e-01</td>\n",
       "      <td>0.928852</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.115950</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.698501e-06</td>\n",
       "      <td>0.203465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.437215e-09</td>\n",
       "      <td>1.528109e-08</td>\n",
       "      <td>0.184108</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.197636</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.319294e-05</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.481720</td>\n",
       "      <td>0.478761</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>4.569686e-06</td>\n",
       "      <td>1.391312e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.003393</td>\n",
       "      <td>5.133540e-05</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.723015e-06</td>\n",
       "      <td>3.776245e-05</td>\n",
       "      <td>0.094714</td>\n",
       "      <td>0.017997</td>\n",
       "      <td>0.015916</td>\n",
       "      <td>2.773921e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123244</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>0.376586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123245</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>0.376586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123246</th>\n",
       "      <td>0.428620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.906781e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.486586e-06</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>2.248993e-02</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123247</th>\n",
       "      <td>0.288678</td>\n",
       "      <td>0.987561</td>\n",
       "      <td>0.033791</td>\n",
       "      <td>2.722036e-03</td>\n",
       "      <td>1.401579e-02</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.033779</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>4.493264e-05</td>\n",
       "      <td>0.720373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.863601e-04</td>\n",
       "      <td>2.160720e-02</td>\n",
       "      <td>0.905640</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.066771</td>\n",
       "      <td>1.111779e-01</td>\n",
       "      <td>2.173034e-02</td>\n",
       "      <td>0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123248</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.052762</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>0.376586</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>3.891505e-06</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1123249 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2     feature_3     feature_4  \\\n",
       "0         0.566236   0.697855   0.009106  0.000000e+00  0.000000e+00   \n",
       "1         0.395012   0.999643   0.012381  0.000000e+00  9.524506e-04   \n",
       "2         0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "3         0.387087   0.560943   0.115950  3.422385e-09  7.597835e-01   \n",
       "4         0.481720   0.478761   0.001350  4.569686e-06  1.391312e-02   \n",
       "...            ...        ...        ...           ...           ...   \n",
       "1123244   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "1123245   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "1123246   0.428620   1.000000   0.509812  0.000000e+00  8.906781e-07   \n",
       "1123247   0.288678   0.987561   0.033791  2.722036e-03  1.401579e-02   \n",
       "1123248   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "\n",
       "         feature_5  feature_6  feature_7  feature_8     feature_9  feature_10  \\\n",
       "0         0.000000   0.000000   0.009106   0.000000  3.972780e-05    0.171641   \n",
       "1         0.000000   0.000000   0.012382   0.000697  1.076902e-04    0.632633   \n",
       "2         0.000000   0.000000   1.000000   0.052762  7.937312e-07    0.376586   \n",
       "3         0.928852   0.000010   0.115950   1.000000  3.698501e-06    0.203465   \n",
       "4         0.000000   0.000000   0.001350   0.003393  5.133540e-05    0.032889   \n",
       "...            ...        ...        ...        ...           ...         ...   \n",
       "1123244   0.000000   0.000000   1.000000   0.052762  7.937312e-07    0.376586   \n",
       "1123245   0.000000   0.000000   1.000000   0.052762  7.937312e-07    0.376586   \n",
       "1123246   0.000000   0.000000   0.509813   0.000001  2.486586e-06    0.601457   \n",
       "1123247   0.000009   0.000208   0.033779   0.001292  4.493264e-05    0.720373   \n",
       "1123248   0.000000   0.000000   1.000000   0.052762  7.937312e-07    0.376586   \n",
       "\n",
       "         feature_11    feature_12    feature_13  feature_14  feature_15  \\\n",
       "0               0.0  9.179891e-05  2.248993e-02    0.680028    1.000000   \n",
       "1               0.0  4.002599e-05  6.486638e-03    0.730756    0.007162   \n",
       "2               0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "3               1.0  3.437215e-09  1.528109e-08    0.184108    0.000010   \n",
       "4               1.0  1.723015e-06  3.776245e-05    0.094714    0.017997   \n",
       "...             ...           ...           ...         ...         ...   \n",
       "1123244         0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "1123245         0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "1123246         0.0  9.179891e-05  2.248993e-02    0.680028    1.000000   \n",
       "1123247         1.0  8.863601e-04  2.160720e-02    0.905640    0.000995   \n",
       "1123248         0.0  1.587743e-08  3.891505e-06    0.580702    0.000543   \n",
       "\n",
       "         feature_16    feature_17    feature_18  feature_19  \n",
       "0          0.459038  6.565172e-07  8.784210e-09    0.000003  \n",
       "1          0.013356  6.395042e-04  0.000000e+00    0.000405  \n",
       "2          1.000000  8.483211e-01  1.026006e-03    0.000000  \n",
       "3          0.197636  1.000000e+00  1.319294e-05    0.000027  \n",
       "4          0.015916  2.773921e-01  0.000000e+00    0.000272  \n",
       "...             ...           ...           ...         ...  \n",
       "1123244    1.000000  8.483211e-01  1.026006e-03    0.000000  \n",
       "1123245    1.000000  8.483211e-01  1.026006e-03    0.000000  \n",
       "1123246    0.459038  6.565172e-07  8.784210e-09    0.000003  \n",
       "1123247    0.066771  1.111779e-01  2.173034e-02    0.000083  \n",
       "1123248    1.000000  8.483211e-01  1.026006e-03    0.000000  \n",
       "\n",
       "[1123249 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = val_processed_data_expanded.drop('encoded_tags', axis=1)\n",
    "y_val = val_processed_data_expanded['encoded_tags']\n",
    "\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f828e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Tag class with below 10 count, double check to make sure\n",
    "counts = y_val.value_counts()\n",
    "\n",
    "y_val = y_val[y_val.isin(counts[counts > 10].index)] # 10 is cutof as mentioned in Preprocessing step\n",
    "X_val = X_val.loc[y_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1519978a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>encoded_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.327324</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.219593</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.189355e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.219594</td>\n",
       "      <td>5.066213e-02</td>\n",
       "      <td>6.685087e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998936</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.153770</td>\n",
       "      <td>5.788697e-01</td>\n",
       "      <td>8.198362e-01</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327324</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.219593</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.189355e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.219594</td>\n",
       "      <td>5.066213e-02</td>\n",
       "      <td>6.685087e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998936</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.153770</td>\n",
       "      <td>5.788697e-01</td>\n",
       "      <td>8.198362e-01</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.268661</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.806285e-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.011853</td>\n",
       "      <td>1.266585e-07</td>\n",
       "      <td>1.335921e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.040388e-04</td>\n",
       "      <td>0.028460</td>\n",
       "      <td>0.859879</td>\n",
       "      <td>0.242375</td>\n",
       "      <td>0.361443</td>\n",
       "      <td>3.157241e-02</td>\n",
       "      <td>6.552889e-06</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.428620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.906781e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>1.013268e-06</td>\n",
       "      <td>2.486586e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.387329</td>\n",
       "      <td>0.612336</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.416578e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.099598e-05</td>\n",
       "      <td>0.008395</td>\n",
       "      <td>0.300106</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.597327e-07</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149097</th>\n",
       "      <td>0.296351</td>\n",
       "      <td>0.983435</td>\n",
       "      <td>0.160314</td>\n",
       "      <td>4.718805e-03</td>\n",
       "      <td>4.475495e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.160314</td>\n",
       "      <td>1.199722e-03</td>\n",
       "      <td>9.322364e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.734385e-03</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.706279</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.130509</td>\n",
       "      <td>3.088446e-02</td>\n",
       "      <td>4.192803e-07</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149098</th>\n",
       "      <td>0.668298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.884876e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.276211e-02</td>\n",
       "      <td>7.937312e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.587743e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.483211e-01</td>\n",
       "      <td>1.026006e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149099</th>\n",
       "      <td>0.387087</td>\n",
       "      <td>0.560943</td>\n",
       "      <td>0.115950</td>\n",
       "      <td>3.422385e-09</td>\n",
       "      <td>7.597835e-01</td>\n",
       "      <td>0.928852</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.115950</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.698501e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.448698e-08</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.208585</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.176432</td>\n",
       "      <td>8.838643e-01</td>\n",
       "      <td>1.165525e-05</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149100</th>\n",
       "      <td>0.428620</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.906781e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>1.013268e-06</td>\n",
       "      <td>2.486586e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149101</th>\n",
       "      <td>0.510120</td>\n",
       "      <td>0.939192</td>\n",
       "      <td>0.067684</td>\n",
       "      <td>3.353653e-11</td>\n",
       "      <td>6.993792e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>1.393255e-02</td>\n",
       "      <td>1.446382e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.179891e-05</td>\n",
       "      <td>0.022490</td>\n",
       "      <td>0.680028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459038</td>\n",
       "      <td>6.565172e-07</td>\n",
       "      <td>8.784210e-09</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1149102 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2     feature_3     feature_4  \\\n",
       "0         0.327324   0.999999   0.219593  1.000000e+00  4.189355e-01   \n",
       "1         0.327324   0.999999   0.219593  1.000000e+00  4.189355e-01   \n",
       "2         0.268661   0.999726   0.011852  0.000000e+00  4.806285e-08   \n",
       "3         0.428620   1.000000   0.509812  0.000000e+00  8.906781e-07   \n",
       "4         0.387329   0.612336   0.001213  0.000000e+00  0.000000e+00   \n",
       "...            ...        ...        ...           ...           ...   \n",
       "1149097   0.296351   0.983435   0.160314  4.718805e-03  4.475495e-02   \n",
       "1149098   0.668298   1.000000   1.000000  6.884876e-09  1.000000e+00   \n",
       "1149099   0.387087   0.560943   0.115950  3.422385e-09  7.597835e-01   \n",
       "1149100   0.428620   1.000000   0.509812  0.000000e+00  8.906781e-07   \n",
       "1149101   0.510120   0.939192   0.067684  3.353653e-11  6.993792e-03   \n",
       "\n",
       "         feature_5  feature_6  feature_7     feature_8     feature_9  ...  \\\n",
       "0         0.000000    0.00000   0.219594  5.066213e-02  6.685087e-06  ...   \n",
       "1         0.000000    0.00000   0.219594  5.066213e-02  6.685087e-06  ...   \n",
       "2         0.000000    0.00000   0.011853  1.266585e-07  1.335921e-04  ...   \n",
       "3         0.000000    0.00000   0.509813  1.013268e-06  2.486586e-06  ...   \n",
       "4         0.000000    0.00000   0.001214  0.000000e+00  4.416578e-04  ...   \n",
       "...            ...        ...        ...           ...           ...  ...   \n",
       "1149097   0.000000    0.00000   0.160314  1.199722e-03  9.322364e-06  ...   \n",
       "1149098   0.000000    0.00000   1.000000  5.276211e-02  7.937312e-07  ...   \n",
       "1149099   0.928852    0.00001   0.115950  1.000000e+00  3.698501e-06  ...   \n",
       "1149100   0.000000    0.00000   0.509813  1.013268e-06  2.486586e-06  ...   \n",
       "1149101   0.000000    1.00000   0.007203  1.393255e-02  1.446382e-05  ...   \n",
       "\n",
       "         feature_11    feature_12  feature_13  feature_14  feature_15  \\\n",
       "0               0.0  1.000000e+00    1.000000    0.998936    0.001565   \n",
       "1               0.0  1.000000e+00    1.000000    0.998936    0.001565   \n",
       "2               0.0  1.040388e-04    0.028460    0.859879    0.242375   \n",
       "3               0.0  9.179891e-05    0.022490    0.680028    1.000000   \n",
       "4               0.0  1.099598e-05    0.008395    0.300106    0.000536   \n",
       "...             ...           ...         ...         ...         ...   \n",
       "1149097         1.0  4.734385e-03    0.001017    0.706279    0.000131   \n",
       "1149098         0.0  1.587743e-08    0.000004    0.580702    0.000543   \n",
       "1149099         1.0  2.448698e-08    0.000063    0.208585    0.000009   \n",
       "1149100         0.0  9.179891e-05    0.022490    0.680028    1.000000   \n",
       "1149101         0.0  9.179891e-05    0.022490    0.680028    1.000000   \n",
       "\n",
       "         feature_16    feature_17    feature_18  feature_19  encoded_tags  \n",
       "0          0.153770  5.788697e-01  8.198362e-01    0.000041           124  \n",
       "1          0.153770  5.788697e-01  8.198362e-01    0.000041           124  \n",
       "2          0.361443  3.157241e-02  6.552889e-06    0.000009           194  \n",
       "3          0.459038  6.565172e-07  8.784210e-09    0.000003           184  \n",
       "4          0.002547  0.000000e+00  4.597327e-07    0.002466           214  \n",
       "...             ...           ...           ...         ...           ...  \n",
       "1149097    0.130509  3.088446e-02  4.192803e-07    0.000040           106  \n",
       "1149098    1.000000  8.483211e-01  1.026006e-03    0.000000           203  \n",
       "1149099    0.176432  8.838643e-01  1.165525e-05    0.000031            99  \n",
       "1149100    0.459038  6.565172e-07  8.784210e-09    0.000003           184  \n",
       "1149101    0.459038  6.565172e-07  8.784210e-09    0.000003           184  \n",
       "\n",
       "[1149102 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_processed_data_expanded = pd.read_csv('./output/scaled_test_data.csv')\n",
    "# train_processed_data_expanded = train_processed_data_expanded.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "test_processed_data_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531e37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the target\n",
    "# scale_pos_weight = len(processed_data_expanded[processed_data_expanded['encoded_label'] == 1]) / len(processed_data_expanded[processed_data_expanded['encoded_label'] == 0])\n",
    "X_train = train_processed_data_expanded.drop('encoded_tags', axis=1)\n",
    "y_train = train_processed_data_expanded['encoded_tags']\n",
    "\n",
    "X_test = test_processed_data_expanded.drop('encoded_tags', axis=1)\n",
    "y_test = test_processed_data_expanded['encoded_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a5ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the scale_pos_weight if you haven't done it yet\n",
    "# scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "# scale_pos_weight \n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = y_train.value_counts().min() / y_train.value_counts()\n",
    "\n",
    "# Convert to the dataset\n",
    "dtrain = lgb.Dataset(X_train, label=y_train, weight=y_train.map(class_weights))\n",
    "dtest = lgb.Dataset(X_test, label=y_test, weight=y_test.map(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a68f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.8, learning_rate=0.01, min_child_weight=1,\n",
       "               n_estimators=1000, objective='binary',\n",
       "               scale_pos_weight=0.03212176665027204, subsample=0.8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create the model\n",
    "# # Calculate weights\n",
    "# weights = np.where(y_train == 0, 69, 0.1)\n",
    "\n",
    "# # lgbm = lgb.LGBMClassifier( is_unbalance=True) # 0.99 scale_pos_weight scale_pos_weight=9\n",
    "# lgbm = lgb.LGBMClassifier(objective='binary',\n",
    "#                           scale_pos_weight=scale_pos_weight, # 69\n",
    "# #                           is_unbalance=True,\n",
    "# #                           max_depth=6,\n",
    "#                           min_child_weight=1,\n",
    "#                           subsample=0.8,\n",
    "#                           colsample_bytree=0.8,\n",
    "#                           learning_rate=0.01,\n",
    "#                           n_estimators=1000)\n",
    "\n",
    "# # Fit the model\n",
    "# lgbm.fit(X_train, y_train)#, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b52b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm.booster_.save_model('./temp/lgbm_model.txt') # lgbm_model_69 lgbm_model_019 lgbm_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc962aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_processed_data_expanded.to_csv(\"./temp/scaled_train_one_col.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e7227",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013e0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3332\n",
      "[LightGBM] [Info] Number of data points in the train set: 5388335, number of used features: 20\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 17 dense feature groups (102.77 MB) transferred to GPU in 0.110183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n",
      "[LightGBM] [Info] Start training from score -5.402677\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "data = dtrain\n",
    "\n",
    "# Define parameters of the already trained model\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',             # memory issue\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y_train)),\n",
    "#     'scale_pos_weight': scale_pos_weight, \n",
    "#     'min_child_weight': 5,\n",
    "    'subsample': 0.8,                    # memory issue\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.05,\n",
    "#     'n_estimators': 1000,                 # memory issue\n",
    "#     'max_bin': 63,                       # reduced for less memory usage\n",
    "#     'num_leaves': 31,                    # memory issue\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "# Training the model --> 3 round takes 10min\n",
    "# num_boost_round = len(cv_results['multi_error-mean'])\n",
    "bst = lgb.train(params, data, num_boost_round=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "202f315c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([124, 124, 194, ...,  99, 184, 184], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "\n",
    "# get the class with the highest probability for each instance\n",
    "# Predict the class probabilities on the test set\n",
    "# 3: match with XGBoost, 95 precision, 91 recall, 95 ROC AUC --> as good as XGBoost 5r\n",
    "y_pred_prob = bst.predict(X_test)\n",
    "y_pred_max_prob = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "y_pred_max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bea8d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3486\n",
      "           1       1.00      1.00      1.00      4293\n",
      "           2       1.00      1.00      1.00      2190\n",
      "           3       1.00      0.50      0.67         2\n",
      "           4       1.00      1.00      1.00       281\n",
      "           5       1.00      1.00      1.00      1265\n",
      "           6       1.00      0.99      1.00       381\n",
      "           7       1.00      1.00      1.00      4526\n",
      "           8       1.00      1.00      1.00        19\n",
      "           9       1.00      0.90      0.95      3493\n",
      "          10       1.00      1.00      1.00        31\n",
      "          11       1.00      1.00      1.00      2601\n",
      "          12       1.00      1.00      1.00        94\n",
      "          13       1.00      1.00      1.00        26\n",
      "          14       1.00      0.99      0.99      4877\n",
      "          15       1.00      1.00      1.00       743\n",
      "          16       1.00      1.00      1.00       348\n",
      "          17       0.99      1.00      1.00       284\n",
      "          18       1.00      0.66      0.80      5847\n",
      "          19       1.00      1.00      1.00       859\n",
      "          20       1.00      1.00      1.00     12421\n",
      "          21       1.00      1.00      1.00      7143\n",
      "          22       1.00      1.00      1.00       268\n",
      "          23       1.00      0.99      0.99      1845\n",
      "          24       1.00      1.00      1.00         9\n",
      "          25       1.00      0.96      0.98        70\n",
      "          26       1.00      1.00      1.00        78\n",
      "          27       1.00      1.00      1.00       183\n",
      "          28       1.00      1.00      1.00       125\n",
      "          29       1.00      1.00      1.00       278\n",
      "          30       1.00      1.00      1.00         8\n",
      "          31       1.00      1.00      1.00        76\n",
      "          32       1.00      1.00      1.00        13\n",
      "          33       1.00      1.00      1.00     10590\n",
      "          34       1.00      1.00      1.00         5\n",
      "          35       1.00      1.00      1.00        15\n",
      "          36       1.00      1.00      1.00       849\n",
      "          37       1.00      1.00      1.00      1094\n",
      "          38       1.00      0.99      1.00      2601\n",
      "          39       1.00      1.00      1.00        28\n",
      "          40       1.00      1.00      1.00       201\n",
      "          41       1.00      0.00      0.01      3006\n",
      "          42       1.00      1.00      1.00      7326\n",
      "          43       1.00      1.00      1.00        24\n",
      "          44       1.00      1.00      1.00         5\n",
      "          45       1.00      0.97      0.99       329\n",
      "          46       0.00      0.00      0.00         3\n",
      "          47       1.00      1.00      1.00       432\n",
      "          48       0.00      0.00      0.00         5\n",
      "          49       1.00      1.00      1.00         5\n",
      "          50       1.00      1.00      1.00       333\n",
      "          51       1.00      0.83      0.91         6\n",
      "          52       0.00      0.00      0.00       119\n",
      "          53       1.00      0.96      0.98       355\n",
      "          54       1.00      0.99      1.00       399\n",
      "          55       1.00      1.00      1.00      6392\n",
      "          56       1.00      1.00      1.00       360\n",
      "          57       1.00      1.00      1.00      9556\n",
      "          58       1.00      1.00      1.00        63\n",
      "          59       1.00      1.00      1.00     10016\n",
      "          60       1.00      0.98      0.99     13416\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       0.55      1.00      0.71         6\n",
      "          63       1.00      0.30      0.47       135\n",
      "          64       1.00      1.00      1.00       240\n",
      "          65       1.00      1.00      1.00     32139\n",
      "          66       1.00      0.99      1.00       137\n",
      "          67       1.00      0.99      1.00       178\n",
      "          68       1.00      1.00      1.00      1294\n",
      "          69       1.00      0.98      0.99      1394\n",
      "          70       1.00      1.00      1.00        75\n",
      "          71       1.00      1.00      1.00        26\n",
      "          72       1.00      0.62      0.77      1374\n",
      "          73       0.20      1.00      0.33         9\n",
      "          74       1.00      0.41      0.58      1797\n",
      "          75       1.00      0.93      0.97     25209\n",
      "          76       0.53      1.00      0.69        19\n",
      "          77       1.00      1.00      1.00       137\n",
      "          78       1.00      1.00      1.00      1719\n",
      "          79       1.00      1.00      1.00         3\n",
      "          80       1.00      1.00      1.00        35\n",
      "          81       1.00      1.00      1.00       237\n",
      "          82       1.00      1.00      1.00        45\n",
      "          83       1.00      1.00      1.00       131\n",
      "          84       1.00      0.96      0.98       178\n",
      "          85       1.00      1.00      1.00       535\n",
      "          86       0.94      0.50      0.65        30\n",
      "          87       1.00      1.00      1.00     23293\n",
      "          88       1.00      1.00      1.00        13\n",
      "          89       1.00      1.00      1.00        43\n",
      "          90       1.00      1.00      1.00      1348\n",
      "          91       1.00      1.00      1.00        93\n",
      "          92       1.00      1.00      1.00        16\n",
      "          93       1.00      1.00      1.00         9\n",
      "          94       1.00      0.99      1.00      1113\n",
      "          95       1.00      0.99      1.00       495\n",
      "          96       1.00      1.00      1.00        30\n",
      "          97       1.00      1.00      1.00        90\n",
      "          98       1.00      1.00      1.00      1534\n",
      "          99       1.00      1.00      1.00     13899\n",
      "         100       1.00      1.00      1.00       296\n",
      "         101       1.00      1.00      1.00        13\n",
      "         102       1.00      0.92      0.96     10459\n",
      "         103       1.00      1.00      1.00       926\n",
      "         104       1.00      1.00      1.00        10\n",
      "         105       1.00      0.27      0.42       198\n",
      "         106       1.00      1.00      1.00     38278\n",
      "         107       0.00      1.00      0.00         2\n",
      "         108       1.00      1.00      1.00        38\n",
      "         109       1.00      1.00      1.00        20\n",
      "         110       1.00      1.00      1.00       387\n",
      "         111       1.00      0.99      1.00      1240\n",
      "         112       1.00      1.00      1.00       195\n",
      "         113       1.00      0.99      1.00      1072\n",
      "         114       1.00      1.00      1.00       114\n",
      "         115       1.00      1.00      1.00       115\n",
      "         116       1.00      1.00      1.00     10724\n",
      "         117       1.00      0.70      0.82        80\n",
      "         118       1.00      1.00      1.00      6304\n",
      "         119       1.00      1.00      1.00      7617\n",
      "         120       1.00      1.00      1.00       181\n",
      "         121       1.00      1.00      1.00        13\n",
      "         122       0.78      1.00      0.88        32\n",
      "         123       1.00      1.00      1.00        18\n",
      "         124       1.00      1.00      1.00     52604\n",
      "         125       1.00      1.00      1.00       751\n",
      "         126       1.00      1.00      1.00       950\n",
      "         127       1.00      1.00      1.00        10\n",
      "         128       1.00      1.00      1.00        17\n",
      "         129       1.00      0.69      0.81      6771\n",
      "         130       1.00      1.00      1.00      3945\n",
      "         131       1.00      1.00      1.00       138\n",
      "         132       0.00      0.00      0.00       139\n",
      "         133       1.00      1.00      1.00         3\n",
      "         134       1.00      1.00      1.00       206\n",
      "         135       1.00      1.00      1.00        29\n",
      "         136       1.00      0.93      0.97       104\n",
      "         137       1.00      1.00      1.00       188\n",
      "         138       1.00      0.81      0.90      3609\n",
      "         139       1.00      1.00      1.00        36\n",
      "         140       1.00      1.00      1.00       176\n",
      "         141       1.00      1.00      1.00      4625\n",
      "         142       1.00      0.96      0.98     25145\n",
      "         143       0.99      1.00      1.00      6786\n",
      "         144       1.00      1.00      1.00         3\n",
      "         145       1.00      1.00      1.00        39\n",
      "         146       1.00      1.00      1.00       230\n",
      "         147       1.00      0.99      1.00      1466\n",
      "         148       1.00      0.93      0.96       136\n",
      "         149       1.00      0.05      0.09        21\n",
      "         150       1.00      1.00      1.00        22\n",
      "         151       1.00      1.00      1.00        71\n",
      "         152       1.00      0.27      0.43       223\n",
      "         153       1.00      0.13      0.23        38\n",
      "         154       1.00      1.00      1.00     13840\n",
      "         155       1.00      1.00      1.00      6249\n",
      "         156       1.00      1.00      1.00         2\n",
      "         157       1.00      0.36      0.53       295\n",
      "         158       1.00      1.00      1.00      5170\n",
      "         159       1.00      1.00      1.00         4\n",
      "         160       1.00      1.00      1.00      1666\n",
      "         161       1.00      1.00      1.00       547\n",
      "         162       0.00      0.00      0.00       801\n",
      "         163       0.99      1.00      1.00       150\n",
      "         164       1.00      1.00      1.00         6\n",
      "         165       1.00      1.00      1.00       562\n",
      "         166       0.99      0.99      0.99       181\n",
      "         167       1.00      1.00      1.00         6\n",
      "         168       1.00      0.83      0.91         6\n",
      "         169       1.00      1.00      1.00       303\n",
      "         170       1.00      0.40      0.57       128\n",
      "         171       1.00      1.00      1.00       158\n",
      "         172       1.00      1.00      1.00        14\n",
      "         173       1.00      0.85      0.92       156\n",
      "         174       1.00      1.00      1.00       384\n",
      "         175       1.00      1.00      1.00       199\n",
      "         176       1.00      1.00      1.00         7\n",
      "         177       1.00      0.19      0.32       315\n",
      "         178       1.00      1.00      1.00         6\n",
      "         179       1.00      1.00      1.00        66\n",
      "         180       1.00      1.00      1.00      4415\n",
      "         181       1.00      1.00      1.00      2235\n",
      "         182       0.60      1.00      0.75         3\n",
      "         183       1.00      0.99      1.00      5989\n",
      "         184       1.00      1.00      1.00    189368\n",
      "         185       1.00      1.00      1.00       137\n",
      "         186       1.00      1.00      1.00       529\n",
      "         187       0.50      1.00      0.67         3\n",
      "         188       1.00      0.20      0.33         5\n",
      "         189       1.00      1.00      1.00      5814\n",
      "         190       1.00      1.00      1.00         5\n",
      "         191       1.00      1.00      1.00         2\n",
      "         192       1.00      1.00      1.00         5\n",
      "         193       1.00      1.00      1.00       328\n",
      "         194       1.00      1.00      1.00    145887\n",
      "         195       1.00      1.00      1.00        10\n",
      "         196       1.00      0.64      0.78      5013\n",
      "         197       1.00      1.00      1.00       142\n",
      "         198       1.00      1.00      1.00       142\n",
      "         199       1.00      1.00      1.00        25\n",
      "         200       0.81      0.98      0.89       202\n",
      "         201       1.00      1.00      1.00        83\n",
      "         202       1.00      1.00      1.00       173\n",
      "         203       1.00      1.00      1.00    240741\n",
      "         204       1.00      1.00      1.00         2\n",
      "         205       1.00      1.00      1.00       320\n",
      "         206       1.00      0.59      0.74      5040\n",
      "         207       1.00      1.00      1.00        18\n",
      "         208       1.00      0.98      0.99        66\n",
      "         209       1.00      1.00      1.00      8177\n",
      "         210       1.00      0.99      1.00       187\n",
      "         211       1.00      1.00      1.00     65211\n",
      "         212       1.00      1.00      1.00     14220\n",
      "         213       1.00      0.93      0.96      1152\n",
      "         214       1.00      1.00      1.00       302\n",
      "         215       1.00      1.00      1.00         4\n",
      "         216       1.00      1.00      1.00       226\n",
      "         217       1.00      1.00      1.00         8\n",
      "         218       0.00      0.00      0.00        27\n",
      "         219       1.00      1.00      1.00        47\n",
      "         220       0.98      1.00      0.99        43\n",
      "         221       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.98   1149102\n",
      "   macro avg       0.95      0.91      0.91   1149102\n",
      "weighted avg       1.00      0.98      0.99   1149102\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_max_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa1a067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.95701893477956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Binarize the output\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_test)\n",
    "y_test_pred_bin = lb.transform(y_pred_max_prob)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_bin, y_test_pred_bin, average='macro')\n",
    "print(f\"ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cd5d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x1abe6b600d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.save_model('./output/lgbm_model_005_lr_3r.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce320e",
   "metadata": {},
   "source": [
    "# Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27d70f",
   "metadata": {},
   "source": [
    "Please make sure to save model above, we need to load it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd8feb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.Booster(model_file='./output/lgbm_model_005_lr_3r.txt')# lgbm_model_005_lr_3r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71ca1103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature  Importance\n",
      "16  feature_16        2048\n",
      "14  feature_14        1896\n",
      "7    feature_7        1602\n",
      "19  feature_19        1565\n",
      "0    feature_0        1330\n",
      "1    feature_1        1325\n",
      "12  feature_12        1270\n",
      "9    feature_9        1149\n",
      "2    feature_2        1066\n",
      "13  feature_13        1054\n",
      "10  feature_10        1011\n",
      "15  feature_15        1001\n",
      "6    feature_6         836\n",
      "4    feature_4         652\n",
      "3    feature_3         570\n",
      "17  feature_17         557\n",
      "8    feature_8         463\n",
      "5    feature_5         256\n",
      "18  feature_18         214\n",
      "11  feature_11         115\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance\n",
    "importance = lgbm.feature_importance()\n",
    "\n",
    "# Get feature names\n",
    "feature_name = lgbm.feature_name()\n",
    "\n",
    "# Create a data frame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_name,\n",
    "    'Importance': importance\n",
    "})\n",
    "\n",
    "importance_df = importance_df.sort_values('Importance', ascending = False)\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0b507",
   "metadata": {},
   "source": [
    "f16: NumWithdrawalsPerMonth --> tag cluster\n",
    "\n",
    "f14: StdDevBalance --> tag cluster\n",
    "\n",
    "f7: num_deposits_sum --> address\n",
    "\n",
    "f19 : MeanSentETHPerLifetime --> tag cluster\n",
    "\n",
    "f0: block_timestamp_min --> address\n",
    "\n",
    "f1: block_timestamp_max --> address\n",
    "\n",
    "f12: MeanNumTransactions --> tag cluster\n",
    "\n",
    "f9: mean_time_between_transactions --> address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a09ca",
   "metadata": {},
   "source": [
    "The distribution of importance is more address/cluster balance, XGBoost based heavily on tag_cluster related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85fda393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.21686223e-05, 1.21686038e-05, 1.21687062e-05, ...,\n",
       "        1.21676996e-05, 1.14017618e-05, 1.21663961e-05],\n",
       "       [1.23054865e-05, 1.23465295e-05, 1.23465834e-05, ...,\n",
       "        1.23463807e-05, 1.23465645e-05, 1.23463543e-05],\n",
       "       [1.93901511e-05, 1.23036337e-05, 1.23033709e-05, ...,\n",
       "        1.23044870e-05, 1.17257986e-05, 1.23020738e-05],\n",
       "       ...,\n",
       "       [1.23235956e-05, 1.23235769e-05, 1.23236806e-05, ...,\n",
       "        1.23236635e-05, 1.17436782e-05, 1.23195015e-05],\n",
       "       [1.23459085e-05, 1.23459860e-05, 1.23455340e-05, ...,\n",
       "        1.23459754e-05, 1.23458368e-05, 1.23458630e-05],\n",
       "       [1.93901511e-05, 1.23036337e-05, 1.23033709e-05, ...,\n",
       "        1.23044870e-05, 1.17257986e-05, 1.23020738e-05]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_test_pred = lgbm.predict(X_val) # X_val X_test\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0081d625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([184, 180, 203, ..., 184,  75, 203], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Do this if load model from txt file\n",
    "y_pred_class = np.argmax(y_test_pred, axis=1) #(y_test_pred > 0.5).astype(int)\n",
    "\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "949eeb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3407\n",
      "           1       1.00      1.00      1.00      4197\n",
      "           2       1.00      1.00      1.00      2141\n",
      "           4       1.00      1.00      1.00       275\n",
      "           5       1.00      1.00      1.00      1237\n",
      "           6       1.00      0.98      0.99       372\n",
      "           7       1.00      1.00      1.00      4424\n",
      "           8       1.00      1.00      1.00        18\n",
      "           9       1.00      0.89      0.94      3414\n",
      "          10       1.00      1.00      1.00        30\n",
      "          11       1.00      1.00      1.00      2543\n",
      "          12       1.00      1.00      1.00        92\n",
      "          13       1.00      1.00      1.00        26\n",
      "          14       1.00      0.99      0.99      4768\n",
      "          15       1.00      1.00      1.00       727\n",
      "          16       1.00      1.00      1.00       340\n",
      "          17       0.99      1.00      0.99       278\n",
      "          18       1.00      0.67      0.80      5715\n",
      "          19       1.00      1.00      1.00       840\n",
      "          20       1.00      1.00      1.00     12142\n",
      "          21       1.00      1.00      1.00      6983\n",
      "          22       1.00      1.00      1.00       262\n",
      "          23       1.00      0.98      0.99      1804\n",
      "          25       1.00      0.93      0.96        68\n",
      "          26       1.00      1.00      1.00        76\n",
      "          27       1.00      1.00      1.00       178\n",
      "          28       1.00      1.00      1.00       122\n",
      "          29       1.00      1.00      1.00       272\n",
      "          31       1.00      1.00      1.00        74\n",
      "          32       1.00      1.00      1.00        13\n",
      "          33       1.00      1.00      1.00     10352\n",
      "          35       1.00      1.00      1.00        15\n",
      "          36       1.00      1.00      1.00       830\n",
      "          37       1.00      1.00      1.00      1069\n",
      "          38       1.00      1.00      1.00      2543\n",
      "          39       1.00      1.00      1.00        27\n",
      "          40       1.00      1.00      1.00       197\n",
      "          41       1.00      0.00      0.01      2938\n",
      "          42       1.00      1.00      1.00      7161\n",
      "          43       1.00      1.00      1.00        23\n",
      "          45       1.00      0.99      1.00       322\n",
      "          47       1.00      1.00      1.00       422\n",
      "          50       1.00      1.00      1.00       326\n",
      "          52       0.00      0.00      0.00       116\n",
      "          53       1.00      0.97      0.99       347\n",
      "          54       1.00      1.00      1.00       390\n",
      "          55       1.00      1.00      1.00      6248\n",
      "          56       1.00      1.00      1.00       351\n",
      "          57       1.00      1.00      1.00      9342\n",
      "          58       1.00      1.00      1.00        62\n",
      "          59       1.00      1.00      1.00      9790\n",
      "          60       1.00      0.98      0.99     13114\n",
      "          62       0.00      0.00      0.00         0\n",
      "          63       1.00      0.36      0.53       132\n",
      "          64       1.00      1.00      1.00       234\n",
      "          65       1.00      1.00      1.00     31416\n",
      "          66       1.00      1.00      1.00       134\n",
      "          67       1.00      0.99      1.00       174\n",
      "          68       1.00      1.00      1.00      1265\n",
      "          69       1.00      0.98      0.99      1362\n",
      "          70       1.00      1.00      1.00        74\n",
      "          71       1.00      1.00      1.00        25\n",
      "          72       1.00      0.62      0.77      1343\n",
      "          73       0.00      0.00      0.00         0\n",
      "          74       1.00      0.43      0.60      1757\n",
      "          75       1.00      0.93      0.96     24642\n",
      "          76       0.64      1.00      0.78        18\n",
      "          77       1.00      1.00      1.00       134\n",
      "          78       1.00      1.00      1.00      1681\n",
      "          80       1.00      1.00      1.00        35\n",
      "          81       1.00      1.00      1.00       232\n",
      "          82       1.00      1.00      1.00        44\n",
      "          83       1.00      1.00      1.00       128\n",
      "          84       1.00      0.95      0.98       174\n",
      "          85       1.00      1.00      1.00       524\n",
      "          86       1.00      0.45      0.62        29\n",
      "          87       1.00      1.00      1.00     22769\n",
      "          88       1.00      1.00      1.00        13\n",
      "          89       1.00      1.00      1.00        42\n",
      "          90       1.00      1.00      1.00      1318\n",
      "          91       1.00      1.00      1.00        91\n",
      "          92       1.00      1.00      1.00        16\n",
      "          94       1.00      1.00      1.00      1088\n",
      "          95       1.00      0.99      1.00       484\n",
      "          96       1.00      1.00      1.00        29\n",
      "          97       1.00      1.00      1.00        88\n",
      "          98       1.00      1.00      1.00      1499\n",
      "          99       1.00      1.00      1.00     13587\n",
      "         100       1.00      1.00      1.00       289\n",
      "         101       1.00      1.00      1.00        12\n",
      "         102       1.00      0.92      0.96     10224\n",
      "         103       1.00      1.00      1.00       905\n",
      "         105       1.00      0.36      0.53       193\n",
      "         106       1.00      1.00      1.00     37416\n",
      "         107       0.00      0.00      0.00         0\n",
      "         108       1.00      1.00      1.00        37\n",
      "         109       1.00      1.00      1.00        19\n",
      "         110       1.00      1.00      1.00       378\n",
      "         111       1.00      1.00      1.00      1212\n",
      "         112       1.00      1.00      1.00       190\n",
      "         113       1.00      1.00      1.00      1048\n",
      "         114       1.00      1.00      1.00       111\n",
      "         115       1.00      1.00      1.00       112\n",
      "         116       1.00      1.00      1.00     10482\n",
      "         117       1.00      0.59      0.74        78\n",
      "         118       1.00      1.00      1.00      6162\n",
      "         119       1.00      1.00      1.00      7446\n",
      "         120       1.00      1.00      1.00       177\n",
      "         121       1.00      1.00      1.00        13\n",
      "         122       0.86      1.00      0.93        32\n",
      "         123       1.00      1.00      1.00        17\n",
      "         124       1.00      1.00      1.00     51420\n",
      "         125       1.00      0.99      1.00       734\n",
      "         126       1.00      1.00      1.00       929\n",
      "         128       1.00      1.00      1.00        16\n",
      "         129       1.00      0.68      0.81      6619\n",
      "         130       1.00      1.00      1.00      3857\n",
      "         131       1.00      1.00      1.00       135\n",
      "         132       0.00      0.00      0.00       136\n",
      "         134       1.00      1.00      1.00       202\n",
      "         135       1.00      1.00      1.00        29\n",
      "         136       1.00      0.95      0.97       102\n",
      "         137       1.00      1.00      1.00       184\n",
      "         138       1.00      0.82      0.90      3528\n",
      "         139       1.00      1.00      1.00        35\n",
      "         140       1.00      1.00      1.00       172\n",
      "         141       1.00      1.00      1.00      4521\n",
      "         142       1.00      0.96      0.98     24579\n",
      "         143       0.99      1.00      1.00      6633\n",
      "         145       1.00      1.00      1.00        38\n",
      "         146       1.00      1.00      1.00       225\n",
      "         147       1.00      1.00      1.00      1433\n",
      "         148       1.00      0.86      0.92       133\n",
      "         149       1.00      0.14      0.25        21\n",
      "         150       1.00      1.00      1.00        21\n",
      "         151       1.00      1.00      1.00        69\n",
      "         152       1.00      0.33      0.49       219\n",
      "         153       1.00      0.16      0.28        37\n",
      "         154       1.00      1.00      1.00     13528\n",
      "         155       1.00      1.00      1.00      6109\n",
      "         157       1.00      0.42      0.60       288\n",
      "         158       1.00      1.00      1.00      5054\n",
      "         160       1.00      1.00      1.00      1629\n",
      "         161       1.00      1.00      1.00       535\n",
      "         162       0.00      0.00      0.00       783\n",
      "         163       0.99      1.00      1.00       147\n",
      "         165       1.00      1.00      1.00       550\n",
      "         166       1.00      1.00      1.00       177\n",
      "         169       1.00      1.00      1.00       296\n",
      "         170       1.00      0.44      0.62       126\n",
      "         171       1.00      1.00      1.00       155\n",
      "         172       1.00      1.00      1.00        14\n",
      "         173       1.00      0.78      0.88       152\n",
      "         174       1.00      0.99      1.00       376\n",
      "         175       1.00      1.00      1.00       194\n",
      "         177       1.00      0.21      0.35       308\n",
      "         179       1.00      1.00      1.00        65\n",
      "         180       1.00      1.00      1.00      4316\n",
      "         181       1.00      1.00      1.00      2185\n",
      "         182       0.00      0.00      0.00         0\n",
      "         183       1.00      0.99      1.00      5854\n",
      "         184       1.00      1.00      1.00    185107\n",
      "         185       1.00      1.00      1.00       134\n",
      "         186       1.00      1.00      1.00       518\n",
      "         188       0.00      0.00      0.00         0\n",
      "         189       1.00      1.00      1.00      5683\n",
      "         193       1.00      1.00      1.00       321\n",
      "         194       1.00      1.00      1.00    142605\n",
      "         196       1.00      0.63      0.77      4901\n",
      "         197       1.00      1.00      1.00       139\n",
      "         198       1.00      1.00      1.00       139\n",
      "         199       1.00      1.00      1.00        24\n",
      "         200       0.78      0.99      0.88       198\n",
      "         201       1.00      1.00      1.00        81\n",
      "         202       1.00      1.00      1.00       169\n",
      "         203       1.00      1.00      1.00    235324\n",
      "         205       1.00      1.00      1.00       312\n",
      "         206       1.00      0.60      0.75      4926\n",
      "         207       1.00      1.00      1.00        18\n",
      "         208       1.00      0.98      0.99        64\n",
      "         209       1.00      1.00      1.00      7993\n",
      "         210       1.00      1.00      1.00       183\n",
      "         211       1.00      1.00      1.00     63744\n",
      "         212       1.00      1.00      1.00     13900\n",
      "         213       1.00      0.93      0.97      1126\n",
      "         214       1.00      1.00      1.00       296\n",
      "         216       1.00      1.00      1.00       221\n",
      "         218       0.00      0.00      0.00        26\n",
      "         219       1.00      1.00      1.00        46\n",
      "         220       1.00      1.00      1.00        42\n",
      "\n",
      "    accuracy                           0.98   1123065\n",
      "   macro avg       0.95      0.89      0.91   1123065\n",
      "weighted avg       1.00      0.98      0.99   1123065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_val, y_pred_class))\n",
    "# Note: 3 round: 95 precision, 89 recall, 96 ROC AUC --> On par with XGBoost 5r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56096c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.958897142472568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Binarize the output\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_val)\n",
    "y_test_pred_bin = lb.transform(y_pred_class)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_bin, y_test_pred_bin, average='macro')\n",
    "print(f\"ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d278c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# # Compute ROC curve and ROC area\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_pred_class)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')  # random classifier\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "# plt.savefig('./output/lightGBM.png')\n",
    "# plt.show()\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = y_test_bin.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_test_pred_bin[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot average ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='darkorange', lw=2, label='Average ROC curve (area = %0.2f)' % roc_auc[\"macro\"])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('./output/lightGB_3r.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# # LightGBM\n",
    "# lgb_auprc = average_precision_score(y_test, y_pred_class)\n",
    "# print(f'LightGBM AUPRC: {lgb_auprc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fe29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
