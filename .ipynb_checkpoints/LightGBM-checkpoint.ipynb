{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415e9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ef5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the string column into separate features\n",
    "# def split_string2(text):\n",
    "#     features = text.split(',')\n",
    "#     for i in range(len(features)):\n",
    "#         if i == 0 or i == 1:\n",
    "#             if features[i] != 'nan':\n",
    "#                 timestamp = datetime.strptime(features[i], '%Y-%m-%d %H:%M:%S%z')\n",
    "#                 features[i] = timestamp.timestamp()\n",
    "#             else:\n",
    "#                 features[i] = datetime(2000, 1, 1, 0, 0, 0, tzinfo=None).timestamp()\n",
    "#         else:\n",
    "#             features[i] = float(features[i])\n",
    "#     return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38fedcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the string column into separate features\n",
    "# def split_string(text):\n",
    "#     # Clean up the string by removing unnecessary characters\n",
    "#     string = text.replace('[', '').replace(']', '').replace('\\n', '')\n",
    "\n",
    "#     # Split the string by spaces\n",
    "#     elements = string.split()\n",
    "\n",
    "#     # Convert each element to a float and create a numpy array\n",
    "#     array = np.array([float(element) for element in elements])\n",
    "#     return array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478a5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# # Function to process a single chunk of data\n",
    "# def process_chunk(chunk):\n",
    "#     # Replace \"Legit\" with 1 and \"Dodgy\" with 0 in 'Label' column\n",
    "#     chunk['encoded_label'] = chunk['Label'].replace({'Legit': 1, 'Dodgy': 0})\n",
    "    \n",
    "#     # Get a list of column names to concatenate\n",
    "#     cols_to_concat = [col for col in chunk.columns if col not in ['Label', 'encoded_label']]\n",
    "#     # Concatenate the values in each row of the specified columns, and store the result in a new column called 'concatenated_values'\n",
    "#     chunk['text'] = chunk[cols_to_concat].apply(lambda row: ','.join(str(val) for val in row), axis=1)\n",
    "\n",
    "#     return chunk[['text', 'encoded_label']]\n",
    "\n",
    "# # Read and process the data in chunks\n",
    "# chunksize = 1000000  # Adjust this value based on your available memory\n",
    "# processed_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694dda38",
   "metadata": {},
   "source": [
    "# Run LightGBM first before running below, use if does not have matching input format validation, uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02150222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine relevant columns into a single text column\n",
    "# with pd.read_csv(\"./temp/validation.csv\", chunksize=chunksize) as reader:\n",
    "#     i = 0\n",
    "#     for chunk in reader:\n",
    "#         print (\"----------------Processing chunk: \", i ,\"-----------------------\")\n",
    "#         processed_chunk = process_chunk(chunk)\n",
    "#         processed_data.append(processed_chunk)\n",
    "#         i += 1\n",
    "\n",
    "# print (\"Finish text column generation, combining chunk....\")\n",
    "# # Combine all processed chunks into a single DataFrame\n",
    "# processed_data = pd.concat(processed_data, ignore_index=True)\n",
    "# #     str(new_df['Account Type']) + ' ' + str(new_df['Contract Type']) \n",
    "# # + ' ' + str(new_df['Entity']) + ' ' + str(new_df['Tags'])+ ' '  +\\\n",
    "# #     new_df['value_frame'] + ' ' + new_df['gas_frame'] + ' ' + new_df['gas_price_frame'] + ' ' + new_df['input_frame']+ ' '  +\\\n",
    "# #     new_df['receipt_cumulative_gas_used_frame'] + ' ' + new_df['receipt_gas_used_frame'] + new_df['receipt_contract_address_frame']+ ' '  +\\\n",
    "# #     new_df['block_timestamp_frame'] + ' ' + new_df['block_number_frame'] + ' ' + new_df['max_fee_per_gas_frame'] + ' ' +\\\n",
    "# #     new_df['max_priority_fee_per_gas_frame'] + new_df['transaction_type_frame'] + ' ' + new_df['address_frame']+ ' '  +\\\n",
    "# #     new_df['eth_balance_frame'] + ' ' + new_df['receipt_effective_gas_price_frame']\n",
    "\n",
    "# processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbaec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = processed_data['text'].apply(split_string2)\n",
    "# processed_data['text'] = features\n",
    "# processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for NaN and infinite values\n",
    "# if processed_data.isnull().values.any():\n",
    "#     print(\"There are NaN values in the processed_data dataframe\")\n",
    "    \n",
    "# # Convert it to float32 for Random Forest library\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Convert the \"text\" column to a numpy array\n",
    "# text_array = np.array(list(processed_data[\"text\"]))\n",
    "\n",
    "# # Scale down the values in the array to the range [0, 1]\n",
    "# scaler = MinMaxScaler()\n",
    "# text_array_scaled = scaler.fit_transform(text_array)\n",
    "\n",
    "# # Cast the scaled array to the float32 data type\n",
    "# text_array_scaled = text_array_scaled.astype(np.float32)\n",
    "\n",
    "# # Create a new dataframe with the scaled \"text\" column and the original \"encoded_label\" column\n",
    "# new_df = pd.DataFrame({'text': list(text_array_scaled), 'encoded_label': processed_data['encoded_label']})\n",
    "\n",
    "# # Replace the original \"processed_data\" dataframe with the new dataframe\n",
    "# processed_data = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ee6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We have 22 elements in this order Account Type,Tags,value_frame,gas_frame ,block_timestamp_frame,receipt_cumulative_gas_used_frame,receipt_effective_gas_price_frame ,eth_balance_frame\n",
    "# column_names = [f'feature_{i}' for i in range(0, 22)]\n",
    "\n",
    "# # Create a new DataFrame with separate columns\n",
    "# encoded_labels = processed_data[\"encoded_label\"]\n",
    "# processed_data_expanded = pd.DataFrame(processed_data['text'].tolist(), columns=column_names, index=processed_data.index)\n",
    "# processed_data_expanded.dropna(inplace=True)\n",
    "# processed_data_expanded[\"encoded_labels\"] = encoded_labels\n",
    "\n",
    "# processed_data_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36071eb6",
   "metadata": {},
   "source": [
    "# If have a saved matching input format validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b612e27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>encoded_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737883</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737884</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737885</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737886</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737887</th>\n",
       "      <td>0.621086</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.135253e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>6.666667e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.022352</td>\n",
       "      <td>0.832683</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>1.160561e-291</td>\n",
       "      <td>0.112382</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1737888 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3     feature_4  feature_5  \\\n",
       "0         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "1         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "2         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "3         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "4         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "...            ...        ...        ...        ...           ...        ...   \n",
       "1737883   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737884   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737885   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737886   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737887   0.621086   0.607200   0.000000        0.0  8.135253e-07        0.0   \n",
       "\n",
       "         feature_6  feature_7     feature_8  feature_9  ...  feature_13  \\\n",
       "0              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "1              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "2              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "3              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "4              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "...            ...        ...           ...        ...  ...         ...   \n",
       "1737883        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737884        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737885        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737886        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737887        0.0   0.000003  6.666667e-07   0.000000  ...    0.000097   \n",
       "\n",
       "         feature_14  feature_15  feature_16  feature_17  feature_18  \\\n",
       "0          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "1          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "2          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "3          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "4          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1737883    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737884    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737885    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737886    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737887    0.000207    0.022352    0.832683    0.000160    0.000230   \n",
       "\n",
       "         feature_19     feature_20  feature_21  encoded_labels  \n",
       "0          0.029047  7.289669e-295    0.000019               1  \n",
       "1          0.029047  7.289669e-295    0.000019               1  \n",
       "2          0.029047  7.289669e-295    0.000019               1  \n",
       "3          0.029047  7.289669e-295    0.000019               1  \n",
       "4          0.029047  7.289669e-295    0.000019               1  \n",
       "...             ...            ...         ...             ...  \n",
       "1737883    0.094337  2.172119e-293    0.000032               1  \n",
       "1737884    0.094337  2.172119e-293    0.000032               1  \n",
       "1737885    0.094337  2.172119e-293    0.000032               1  \n",
       "1737886    0.094337  2.172119e-293    0.000032               1  \n",
       "1737887    0.000031  1.160561e-291    0.112382               1  \n",
       "\n",
       "[1737888 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # train_processed_data = pd.read_csv('./output/non_nan_balanced_data.csv')\n",
    "# processed_data_expanded = pd.read_csv('./output/scaled_to_only_validation_one_col.csv')\n",
    "# # processed_data_expanded = processed_data_expanded.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# processed_data_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fb745da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>encoded_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.630983</td>\n",
       "      <td>0.979582</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606367</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.030820</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.366357</td>\n",
       "      <td>0.029047</td>\n",
       "      <td>7.289669e-295</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737883</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737884</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737885</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737886</th>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.611366</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.016706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.030961</td>\n",
       "      <td>0.241181</td>\n",
       "      <td>0.094337</td>\n",
       "      <td>2.172119e-293</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737887</th>\n",
       "      <td>0.621086</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.135253e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>6.666667e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.022352</td>\n",
       "      <td>0.832683</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>1.160561e-291</td>\n",
       "      <td>0.112382</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1737888 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3     feature_4  feature_5  \\\n",
       "0         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "1         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "2         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "3         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "4         0.630983   0.979582   0.002110        0.0  0.000000e+00        0.0   \n",
       "...            ...        ...        ...        ...           ...        ...   \n",
       "1737883   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737884   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737885   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737886   0.583069   0.611366   0.000025        0.0  0.000000e+00        0.0   \n",
       "1737887   0.621086   0.607200   0.000000        0.0  8.135253e-07        0.0   \n",
       "\n",
       "         feature_6  feature_7     feature_8  feature_9  ...  feature_13  \\\n",
       "0              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "1              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "2              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "3              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "4              0.0   0.002113  0.000000e+00   0.001864  ...    0.606367   \n",
       "...            ...        ...           ...        ...  ...         ...   \n",
       "1737883        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737884        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737885        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737886        0.0   0.000027  0.000000e+00   0.016706  ...    0.270207   \n",
       "1737887        0.0   0.000003  6.666667e-07   0.000000  ...    0.000097   \n",
       "\n",
       "         feature_14  feature_15  feature_16  feature_17  feature_18  \\\n",
       "0          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "1          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "2          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "3          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "4          0.000104    0.030820    0.857497    0.239467    0.366357   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "1737883    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737884    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737885    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737886    0.000017    0.004046    0.665012    0.030961    0.241181   \n",
       "1737887    0.000207    0.022352    0.832683    0.000160    0.000230   \n",
       "\n",
       "         feature_19     feature_20  feature_21  encoded_labels  \n",
       "0          0.029047  7.289669e-295    0.000019               1  \n",
       "1          0.029047  7.289669e-295    0.000019               1  \n",
       "2          0.029047  7.289669e-295    0.000019               1  \n",
       "3          0.029047  7.289669e-295    0.000019               1  \n",
       "4          0.029047  7.289669e-295    0.000019               1  \n",
       "...             ...            ...         ...             ...  \n",
       "1737883    0.094337  2.172119e-293    0.000032               1  \n",
       "1737884    0.094337  2.172119e-293    0.000032               1  \n",
       "1737885    0.094337  2.172119e-293    0.000032               1  \n",
       "1737886    0.094337  2.172119e-293    0.000032               1  \n",
       "1737887    0.000031  1.160561e-291    0.112382               1  \n",
       "\n",
       "[1737888 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # processed_data_expanded.dropna(subset=['Account Type'], inplace=True)\n",
    "# # feature_12 is Tags --> remove\n",
    "# processed_data_expanded = processed_data_expanded.drop('feature_12', axis=1)\n",
    "\n",
    "# # Define the desired column names\n",
    "# new_column_names = ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5',\n",
    "#                     'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11',\n",
    "#                     'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17',\n",
    "#                     'feature_18', 'feature_19', 'encoded_tags']\n",
    "\n",
    "# # Rename the columns\n",
    "# processed_data_expanded.columns = new_column_names\n",
    "\n",
    "# processed_data_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9431e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the features and the target\n",
    "# # scale_pos_weight = len(processed_data_expanded[processed_data_expanded['encoded_label'] == 1]) / len(processed_data_expanded[processed_data_expanded['encoded_label'] == 0])\n",
    "# X_val = processed_data_expanded.drop('encoded_labels', axis=1)\n",
    "# y_val = processed_data_expanded['encoded_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe9d59",
   "metadata": {},
   "source": [
    "# LightGBM, for first run, uncomment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_processed_data = pd.read_csv('./output/non_nan_balanced_data.csv')\n",
    "# train_processed_data = pd.read_csv('./output/one_text_col_data.csv')\n",
    "\n",
    "# train_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c79f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one_text_col_data\n",
    "# features = train_processed_data['text'].apply(split_string2) #split_string2 if one_text_col_data.csv\n",
    "# train_processed_data['text'] = features\n",
    "# train_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26169573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_processed_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6aef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for NaN and infinite values\n",
    "# if train_processed_data.isnull().values.any():\n",
    "#     print(\"There are NaN values in the processed_data dataframe\")\n",
    "    \n",
    "# # Convert it to float32 for Random Forest library\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Convert the \"text\" column to a numpy array\n",
    "# text_array = np.array(list(train_processed_data[\"text\"]))\n",
    "\n",
    "# # Scale down the values in the array to the range [0, 1]\n",
    "# scaler = MinMaxScaler()\n",
    "# text_array_scaled = scaler.fit_transform(text_array)\n",
    "\n",
    "# # Cast the scaled array to the float32 data type\n",
    "# text_array_scaled = text_array_scaled.astype(np.float32)\n",
    "\n",
    "# # Create a new dataframe with the scaled \"text\" column and the original \"encoded_label\" column\n",
    "# new_df = pd.DataFrame({'text': list(text_array_scaled), 'encoded_label': train_processed_data['encoded_label']})\n",
    "\n",
    "# # Replace the original \"processed_data\" dataframe with the new dataframe\n",
    "# train_processed_data = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the string column into separate features\n",
    "# def split_string(text):\n",
    "#     # Clean up the string by removing unnecessary characters\n",
    "#     string = text.replace('[', '').replace(']', '').replace('\\n', '')\n",
    "\n",
    "#     # Split the string by spaces\n",
    "#     elements = string.split()\n",
    "\n",
    "#     # Convert each element to a float and create a numpy array\n",
    "#     array = np.array([float(element) for element in elements])\n",
    "#     return array \n",
    "\n",
    "# # processed_data = processed_data['text'].astype(np.float32)\n",
    "# features = train_processed_data['text'].apply(split_string)\n",
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442843e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_processed_data['text'] = features\n",
    "# train_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a85c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We have 22 elements in this order Account Type,Tags,value_frame,gas_frame ,block_timestamp_frame,receipt_cumulative_gas_used_frame,receipt_effective_gas_price_frame ,eth_balance_frame\n",
    "# train_column_names = [f'feature_{i}' for i in range(0, 22)]\n",
    "\n",
    "# # Create a new DataFrame with separate columns\n",
    "# train_encoded_labels = train_processed_data[\"encoded_label\"]\n",
    "# train_processed_data_expanded = pd.DataFrame(train_processed_data['text'].tolist(), columns=train_column_names, index=train_processed_data.index)\n",
    "# train_processed_data_expanded.dropna(inplace=True)\n",
    "# train_processed_data_expanded[\"encoded_labels\"] = train_encoded_labels\n",
    "\n",
    "# train_processed_data_expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7adae07",
   "metadata": {},
   "source": [
    "# End LightGBM if run first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8892f3cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>encoded_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.623455</td>\n",
       "      <td>0.997611</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.501077e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>7.599510e-07</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.623455</td>\n",
       "      <td>0.997611</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.501077e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>7.599510e-07</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.623455</td>\n",
       "      <td>0.997611</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.501077e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>7.599510e-07</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.623455</td>\n",
       "      <td>0.997611</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.501077e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>7.599510e-07</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.623455</td>\n",
       "      <td>0.997611</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.501077e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>7.599510e-07</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915131</th>\n",
       "      <td>0.354652</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.021302</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>5.835128e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351985</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>6.332925e-05</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.680482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.465448</td>\n",
       "      <td>7.123401e-07</td>\n",
       "      <td>2.652135e-296</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915132</th>\n",
       "      <td>0.354652</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.021302</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>5.835128e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351985</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>6.332925e-05</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915133</th>\n",
       "      <td>0.354652</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.021302</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>5.835128e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351985</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>6.332925e-05</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915134</th>\n",
       "      <td>0.354652</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.021302</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>5.835128e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351985</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>6.332925e-05</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5915135</th>\n",
       "      <td>0.354652</td>\n",
       "      <td>0.999975</td>\n",
       "      <td>0.021302</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>5.835128e-05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.351985</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>6.332925e-05</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.027793</td>\n",
       "      <td>0.860078</td>\n",
       "      <td>0.239291</td>\n",
       "      <td>0.361326</td>\n",
       "      <td>2.854600e-02</td>\n",
       "      <td>1.905827e-293</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5915136 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0  feature_1  feature_2  feature_3     feature_4  feature_5  \\\n",
       "0         0.623455   0.997611   0.002130   0.000000  3.501077e-07        0.0   \n",
       "1         0.623455   0.997611   0.002130   0.000000  3.501077e-07        0.0   \n",
       "2         0.623455   0.997611   0.002130   0.000000  3.501077e-07        0.0   \n",
       "3         0.623455   0.997611   0.002130   0.000000  3.501077e-07        0.0   \n",
       "4         0.623455   0.997611   0.002130   0.000000  3.501077e-07        0.0   \n",
       "...            ...        ...        ...        ...           ...        ...   \n",
       "5915131   0.354652   0.999975   0.021302   0.000057  5.835128e-05        0.0   \n",
       "5915132   0.354652   0.999975   0.021302   0.000057  5.835128e-05        0.0   \n",
       "5915133   0.354652   0.999975   0.021302   0.000057  5.835128e-05        0.0   \n",
       "5915134   0.354652   0.999975   0.021302   0.000057  5.835128e-05        0.0   \n",
       "5915135   0.354652   0.999975   0.021302   0.000057  5.835128e-05        0.0   \n",
       "\n",
       "         feature_6  feature_7     feature_8  feature_9  ...  feature_11  \\\n",
       "0         0.000000   0.002130  7.599510e-07   0.000547  ...         0.0   \n",
       "1         0.000000   0.002130  7.599510e-07   0.000547  ...         0.0   \n",
       "2         0.000000   0.002130  7.599510e-07   0.000547  ...         0.0   \n",
       "3         0.000000   0.002130  7.599510e-07   0.000547  ...         0.0   \n",
       "4         0.000000   0.002130  7.599510e-07   0.000547  ...         0.0   \n",
       "...            ...        ...           ...        ...  ...         ...   \n",
       "5915131   0.351985   0.000002  6.332925e-05   0.000094  ...         0.0   \n",
       "5915132   0.351985   0.000002  6.332925e-05   0.000094  ...         0.0   \n",
       "5915133   0.351985   0.000002  6.332925e-05   0.000094  ...         0.0   \n",
       "5915134   0.351985   0.000002  6.332925e-05   0.000094  ...         0.0   \n",
       "5915135   0.351985   0.000002  6.332925e-05   0.000094  ...         0.0   \n",
       "\n",
       "         feature_12  feature_13  feature_14  feature_15  feature_16  \\\n",
       "0          0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "1          0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "2          0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "3          0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "4          0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "...             ...         ...         ...         ...         ...   \n",
       "5915131    0.000091    0.018721    0.680482    1.000000    0.465448   \n",
       "5915132    0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "5915133    0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "5915134    0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "5915135    0.000104    0.027793    0.860078    0.239291    0.361326   \n",
       "\n",
       "           feature_17     feature_18  feature_19  encoded_tags  \n",
       "0        2.854600e-02  1.905827e-293    0.000015           217  \n",
       "1        2.854600e-02  1.905827e-293    0.000015           217  \n",
       "2        2.854600e-02  1.905827e-293    0.000015           217  \n",
       "3        2.854600e-02  1.905827e-293    0.000015           217  \n",
       "4        2.854600e-02  1.905827e-293    0.000015           217  \n",
       "...               ...            ...         ...           ...  \n",
       "5915131  7.123401e-07  2.652135e-296    0.000009           203  \n",
       "5915132  2.854600e-02  1.905827e-293    0.000015           217  \n",
       "5915133  2.854600e-02  1.905827e-293    0.000015           217  \n",
       "5915134  2.854600e-02  1.905827e-293    0.000015           217  \n",
       "5915135  2.854600e-02  1.905827e-293    0.000015           217  \n",
       "\n",
       "[5915136 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_processed_data_expanded = pd.read_csv('./output/scaled_train_data.csv')\n",
    "train_processed_data_expanded = train_processed_data_expanded.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "train_processed_data_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531e37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features and the target\n",
    "# scale_pos_weight = len(processed_data_expanded[processed_data_expanded['encoded_label'] == 1]) / len(processed_data_expanded[processed_data_expanded['encoded_label'] == 0])\n",
    "X = train_processed_data_expanded.drop('encoded_tags', axis=1)\n",
    "y = train_processed_data_expanded['encoded_tags']\n",
    "\n",
    "# Remove the Tag class with below 10 count\n",
    "counts = y.value_counts()\n",
    "y_filtered = y[y.isin(counts[counts > 10].index)] # 10 is cutof as mentioned in Preprocessing step\n",
    "X_filtered = X[y.isin(counts[counts > 10].index)]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_filtered, test_size=0.2, random_state=25, stratify=y_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a5ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the scale_pos_weight if you haven't done it yet\n",
    "# scale_pos_weight = sum(y_train == 0) / sum(y_train == 1)\n",
    "# scale_pos_weight \n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = y_train.value_counts().min() / y_train.value_counts()\n",
    "\n",
    "# Convert the dataset into DMatrix\n",
    "dtrain = lgb.Dataset(X_train, label=y_train, weight=y_train.map(class_weights))\n",
    "dtest = lgb.Dataset(X_test, label=y_test, weight=y_test.map(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a68f6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.8, learning_rate=0.01, min_child_weight=1,\n",
       "               n_estimators=1000, objective='binary',\n",
       "               scale_pos_weight=0.03212176665027204, subsample=0.8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create the model\n",
    "# # Calculate weights\n",
    "# weights = np.where(y_train == 0, 69, 0.1)\n",
    "\n",
    "# # lgbm = lgb.LGBMClassifier( is_unbalance=True) # 0.99 scale_pos_weight scale_pos_weight=9\n",
    "# lgbm = lgb.LGBMClassifier(objective='binary',\n",
    "#                           scale_pos_weight=scale_pos_weight, # 69\n",
    "# #                           is_unbalance=True,\n",
    "# #                           max_depth=6,\n",
    "#                           min_child_weight=1,\n",
    "#                           subsample=0.8,\n",
    "#                           colsample_bytree=0.8,\n",
    "#                           learning_rate=0.01,\n",
    "#                           n_estimators=1000)\n",
    "\n",
    "# # Fit the model\n",
    "# lgbm.fit(X_train, y_train)#, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b52b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm.booster_.save_model('./temp/lgbm_model.txt') # lgbm_model_69 lgbm_model_019 lgbm_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc962aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_processed_data_expanded.to_csv(\"./temp/scaled_train_one_col.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e7227",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "013e0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3212\n",
      "[LightGBM] [Info] Number of data points in the train set: 4732037, number of used features: 19\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060 Laptop GPU, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (72.21 MB) transferred to GPU in 0.104620 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -5.398163\n",
      "[LightGBM] [Info] Start training from score -34.538776\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "data = dtrain\n",
    "\n",
    "# Define parameters of the already trained model\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',             # memory issue\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': len(np.unique(y)),\n",
    "#     'scale_pos_weight': scale_pos_weight, \n",
    "#     'min_child_weight': 5,\n",
    "    'subsample': 0.8,                    # memory issue\n",
    "    'colsample_bytree': 0.8,\n",
    "    'learning_rate': 0.05,\n",
    "#     'n_estimators': 1000,                 # memory issue\n",
    "#     'max_bin': 63,                       # reduced for less memory usage\n",
    "#     'num_leaves': 31,                    # memory issue\n",
    "    'device': 'gpu'\n",
    "}\n",
    "\n",
    "# Cannot run Cross validation due to computational constraint\n",
    "# cv_results = lgb.cv(\n",
    "#     params,\n",
    "#     data,\n",
    "#     num_boost_round=2,  # REDUCE DUE TO MEMORY LIMITATION LOCALLY!!!!!!!!!!!!!!!!!\n",
    "#     nfold=3, # starts with 2, not match XGBoost\n",
    "#     metrics=['multi_error', 'multi_logloss'],\n",
    "#     callbacks=[lgb.callback.log_evaluation(period=1)]\n",
    "# #     verbose_eval=True\n",
    "# )\n",
    "\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "# Training the model\n",
    "# num_boost_round = len(cv_results['multi_error-mean'])\n",
    "bst = lgb.train(params, data, num_boost_round=3) # 3: match with XGBoost, precision 0.99, 1 for others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cd5d2b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x211d2556c70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.save_model('./output/lgbm_model_005_lr.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce320e",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27d70f",
   "metadata": {},
   "source": [
    "Please make sure to save model above, we need to load it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8feb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = lgb.Booster(model_file='./output/lgbm_model_005_lr.txt')# lgbm_model_005_lr lgbm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71ca1103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Feature  Importance\n",
      "19  feature_19        2029\n",
      "0    feature_0        1989\n",
      "7    feature_7        1835\n",
      "16  feature_16        1511\n",
      "14  feature_14        1360\n",
      "10  feature_10        1358\n",
      "9    feature_9        1337\n",
      "1    feature_1        1268\n",
      "15  feature_15        1098\n",
      "17  feature_17        1028\n",
      "12  feature_12         914\n",
      "2    feature_2         912\n",
      "13  feature_13         782\n",
      "6    feature_6         737\n",
      "3    feature_3         618\n",
      "4    feature_4         444\n",
      "8    feature_8         410\n",
      "5    feature_5         163\n",
      "11  feature_11          97\n",
      "18  feature_18           0\n"
     ]
    }
   ],
   "source": [
    "# Get feature importance\n",
    "importance = lgbm.feature_importance()\n",
    "\n",
    "# Get feature names\n",
    "feature_name = lgbm.feature_name()\n",
    "\n",
    "# Create a data frame for visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_name,\n",
    "    'Importance': importance\n",
    "})\n",
    "\n",
    "importance_df = importance_df.sort_values('Importance', ascending = False)\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0b507",
   "metadata": {},
   "source": [
    "f19 : MeanSentETHPerLifetime --> tag cluster\n",
    "\n",
    "f0: block_timestamp_min --> address\n",
    "\n",
    "f7: num_deposits_sum --> address\n",
    "\n",
    "f16: NumWithdrawalsPerMonth --> tag cluster\n",
    "\n",
    "f14: StdDevBalance --> tag cluster\n",
    "    \n",
    "f10: lifetime --> address\n",
    "\n",
    "f9: mean_time_between_transactions --> address\n",
    "\n",
    "f1: block_timestamp_max --> address\n",
    "    \n",
    "f15: ClusterLifetime --> tag cluster\n",
    "\n",
    "f17: NumDepositsPerMonth --> tag cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089a09ca",
   "metadata": {},
   "source": [
    "The distribution of importance is more widespread here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85fda393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.26954083e-05, 1.26953897e-05, 3.25852447e-18, ...,\n",
       "        1.26954412e-05, 1.27015385e-05, 3.25852447e-18],\n",
       "       [1.29057702e-05, 1.29057513e-05, 3.31251797e-18, ...,\n",
       "        1.29058042e-05, 1.29059042e-05, 3.31251797e-18],\n",
       "       [1.29201050e-05, 1.29190161e-05, 3.31624068e-18, ...,\n",
       "        1.42303768e-05, 1.29185919e-05, 3.31624068e-18],\n",
       "       ...,\n",
       "       [1.28659251e-05, 1.28673690e-05, 3.30267019e-18, ...,\n",
       "        1.28661719e-05, 1.28744087e-05, 3.30267019e-18],\n",
       "       [1.26954195e-05, 1.26954009e-05, 3.25852734e-18, ...,\n",
       "        1.26954524e-05, 1.27020961e-05, 3.25852734e-18],\n",
       "       [1.29006702e-05, 1.29035560e-05, 3.31195449e-18, ...,\n",
       "        1.29034375e-05, 1.29022940e-05, 3.31195449e-18]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_test_pred = lgbm.predict(X_test)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0081d625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([217,  63, 226, ..., 117, 217, 203], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Do this if load model from txt file\n",
    "y_pred_class = np.argmax(y_test_pred, axis=1) #(y_test_pred > 0.5).astype(int)\n",
    "# i = 0\n",
    "# for val in y_pred_class:\n",
    "#     if val == 0:\n",
    "#         print (\"Here :\", i)\n",
    "#         break\n",
    "#     i += 1\n",
    "\n",
    "# i = 0\n",
    "# for val in y_val_pred:\n",
    "#     if val < 0.5:\n",
    "#         print (\"Here :\", i)\n",
    "#         break\n",
    "#     i += 1\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "949eeb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3587\n",
      "           1       1.00      1.00      1.00      4402\n",
      "           4       1.00      1.00      1.00      2252\n",
      "           5       1.00      1.00      1.00         2\n",
      "           6       1.00      1.00      1.00       291\n",
      "           7       1.00      1.00      1.00      1291\n",
      "           8       1.00      1.00      1.00       385\n",
      "           9       1.00      1.00      1.00      4660\n",
      "          10       1.00      1.00      1.00        20\n",
      "          11       1.00      1.00      1.00      3572\n",
      "          12       0.94      1.00      0.97        31\n",
      "          13       1.00      1.00      1.00      2626\n",
      "          14       1.00      0.99      0.99        93\n",
      "          15       1.00      1.00      1.00        26\n",
      "          16       1.00      1.00      1.00      4993\n",
      "          17       1.00      1.00      1.00       775\n",
      "          18       1.00      1.00      1.00       354\n",
      "          19       1.00      0.99      1.00       298\n",
      "          20       1.00      1.00      1.00      5995\n",
      "          21       1.00      1.00      1.00       887\n",
      "          22       1.00      1.00      1.00     12797\n",
      "          23       1.00      1.00      1.00      7349\n",
      "          24       1.00      1.00      1.00       272\n",
      "          25       1.00      1.00      1.00      1908\n",
      "          26       1.00      0.91      0.95        11\n",
      "          27       0.99      1.00      0.99        75\n",
      "          28       1.00      1.00      1.00        82\n",
      "          29       1.00      1.00      1.00       186\n",
      "          32       1.00      1.00      1.00       125\n",
      "          33       1.00      1.00      1.00       287\n",
      "          34       1.00      1.00      1.00         8\n",
      "          35       1.00      1.00      1.00        78\n",
      "          36       1.00      1.00      1.00        13\n",
      "          37       1.00      1.00      1.00     10853\n",
      "          39       1.00      1.00      1.00         5\n",
      "          40       1.00      1.00      1.00        15\n",
      "          41       1.00      1.00      1.00       877\n",
      "          42       1.00      1.00      1.00      1118\n",
      "          43       1.00      1.00      1.00      2690\n",
      "          44       1.00      1.00      1.00        28\n",
      "          45       1.00      1.00      1.00       200\n",
      "          46       1.00      1.00      1.00      3471\n",
      "          47       1.00      1.00      1.00      7528\n",
      "          48       1.00      1.00      1.00        24\n",
      "          49       0.83      1.00      0.91         5\n",
      "          50       1.00      1.00      1.00       336\n",
      "          51       1.00      1.00      1.00         3\n",
      "          53       0.99      1.00      0.99       439\n",
      "          54       1.00      1.00      1.00         5\n",
      "          55       1.00      0.50      0.67         4\n",
      "          56       0.98      0.99      0.99       344\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       1.00      0.99      1.00       139\n",
      "          59       1.00      1.00      1.00       360\n",
      "          60       1.00      1.00      1.00       404\n",
      "          61       1.00      1.00      1.00      6568\n",
      "          62       1.00      1.00      1.00       362\n",
      "          63       1.00      1.00      1.00      9839\n",
      "          65       1.00      1.00      1.00        70\n",
      "          66       1.00      1.00      1.00     10297\n",
      "          67       1.00      1.00      1.00     13757\n",
      "          68       1.00      1.00      1.00         5\n",
      "          69       1.00      0.80      0.89         5\n",
      "          70       1.00      1.00      1.00       134\n",
      "          71       1.00      1.00      1.00       249\n",
      "          72       1.00      1.00      1.00     33013\n",
      "          73       1.00      1.00      1.00       144\n",
      "          74       0.99      1.00      1.00       187\n",
      "          75       1.00      1.00      1.00      1310\n",
      "          76       1.00      1.00      1.00      1423\n",
      "          77       1.00      1.00      1.00        79\n",
      "          78       1.00      1.00      1.00        25\n",
      "          79       1.00      1.00      1.00      1412\n",
      "          80       0.13      0.90      0.23        10\n",
      "          81       1.00      1.00      1.00      1837\n",
      "          82       1.00      1.00      1.00     25814\n",
      "          83       1.00      1.00      1.00        21\n",
      "          84       1.00      1.00      1.00       142\n",
      "          85       1.00      1.00      1.00      1749\n",
      "          87       1.00      1.00      1.00         3\n",
      "          88       1.00      1.00      1.00        35\n",
      "          89       1.00      0.99      1.00       248\n",
      "          90       1.00      1.00      1.00        46\n",
      "          91       1.00      1.00      1.00       133\n",
      "          92       0.98      0.99      0.99       184\n",
      "          93       1.00      1.00      1.00       556\n",
      "          94       1.00      0.97      0.98        30\n",
      "          96       1.00      1.00      1.00     23834\n",
      "          97       1.00      1.00      1.00        14\n",
      "          98       1.00      1.00      1.00        44\n",
      "          99       1.00      1.00      1.00      1378\n",
      "         100       1.00      1.00      1.00        94\n",
      "         101       1.00      1.00      1.00        16\n",
      "         102       0.82      1.00      0.90         9\n",
      "         103       1.00      1.00      1.00      1137\n",
      "         104       1.00      1.00      1.00       512\n",
      "         105       1.00      1.00      1.00        29\n",
      "         106       1.00      1.00      1.00        95\n",
      "         107       1.00      1.00      1.00      1583\n",
      "         108       1.00      1.00      1.00     16129\n",
      "         110       1.00      1.00      1.00       309\n",
      "         112       1.00      1.00      1.00        14\n",
      "         113       1.00      1.00      1.00     10742\n",
      "         114       1.00      1.00      1.00       947\n",
      "         115       1.00      1.00      1.00         9\n",
      "         116       1.00      1.00      1.00       200\n",
      "         117       1.00      1.00      1.00     39264\n",
      "         119       1.00      1.00      1.00        37\n",
      "         120       1.00      1.00      1.00        21\n",
      "         121       1.00      1.00      1.00       398\n",
      "         122       1.00      1.00      1.00      1288\n",
      "         123       1.00      1.00      1.00       200\n",
      "         124       1.00      1.00      1.00      1096\n",
      "         125       1.00      1.00      1.00       114\n",
      "         126       1.00      1.00      1.00       115\n",
      "         127       1.00      1.00      1.00     11045\n",
      "         128       1.00      1.00      1.00        83\n",
      "         129       1.00      1.00      1.00      6498\n",
      "         130       1.00      1.00      1.00      7835\n",
      "         131       0.99      1.00      1.00       185\n",
      "         132       1.00      1.00      1.00        14\n",
      "         133       0.97      1.00      0.99        33\n",
      "         134       1.00      1.00      1.00        20\n",
      "         135       1.00      1.00      1.00     53951\n",
      "         136       1.00      1.00      1.00       770\n",
      "         137       1.00      1.00      1.00       979\n",
      "         139       1.00      1.00      1.00        10\n",
      "         140       1.00      1.00      1.00        17\n",
      "         141       1.00      1.00      1.00      6947\n",
      "         143       1.00      1.00      1.00      4066\n",
      "         144       1.00      1.00      1.00       145\n",
      "         145       1.00      1.00      1.00       141\n",
      "         146       1.00      0.67      0.80         3\n",
      "         147       1.00      1.00      1.00       212\n",
      "         148       0.28      1.00      0.44        31\n",
      "         149       1.00      1.00      1.00       102\n",
      "         150       1.00      1.00      1.00       194\n",
      "         151       1.00      1.00      1.00      3716\n",
      "         152       1.00      1.00      1.00        37\n",
      "         153       1.00      1.00      1.00       179\n",
      "         154       1.00      1.00      1.00      4762\n",
      "         156       1.00      1.00      1.00     25819\n",
      "         157       1.00      1.00      1.00      6966\n",
      "         158       1.00      1.00      1.00         3\n",
      "         160       1.00      1.00      1.00        39\n",
      "         161       1.00      1.00      1.00       240\n",
      "         162       1.00      1.00      1.00      1500\n",
      "         164       1.00      1.00      1.00       138\n",
      "         165       1.00      1.00      1.00        23\n",
      "         166       1.00      1.00      1.00        23\n",
      "         167       1.00      1.00      1.00        74\n",
      "         168       1.00      1.00      1.00       257\n",
      "         169       0.96      1.00      0.98        43\n",
      "         170       1.00      1.00      1.00     16063\n",
      "         171       1.00      1.00      1.00      6410\n",
      "         172       0.11      1.00      0.20         3\n",
      "         173       1.00      1.00      1.00       345\n",
      "         175       1.00      1.00      1.00      5287\n",
      "         176       1.00      1.00      1.00         4\n",
      "         177       1.00      1.00      1.00      1715\n",
      "         178       1.00      1.00      1.00       557\n",
      "         179       1.00      1.00      1.00       830\n",
      "         180       0.99      0.99      0.99       152\n",
      "         181       1.00      1.00      1.00         5\n",
      "         182       1.00      1.00      1.00       577\n",
      "         184       1.00      1.00      1.00       184\n",
      "         185       1.00      1.00      1.00         6\n",
      "         186       1.00      1.00      1.00         6\n",
      "         187       0.99      1.00      0.99       317\n",
      "         188       1.00      1.00      1.00       147\n",
      "         189       1.00      1.00      1.00       165\n",
      "         190       1.00      1.00      1.00        15\n",
      "         191       1.00      1.00      1.00       158\n",
      "         192       1.00      1.00      1.00       392\n",
      "         193       1.00      1.00      1.00       205\n",
      "         194       1.00      1.00      1.00         7\n",
      "         195       1.00      1.00      1.00       330\n",
      "         196       0.83      1.00      0.91         5\n",
      "         197       1.00      1.00      1.00        64\n",
      "         198       1.00      1.00      1.00      4520\n",
      "         199       1.00      1.00      1.00      2306\n",
      "         200       1.00      1.00      1.00         3\n",
      "         202       1.00      1.00      1.00      6134\n",
      "         203       1.00      1.00      1.00    193963\n",
      "         204       1.00      1.00      1.00       140\n",
      "         207       1.00      1.00      1.00       541\n",
      "         209       0.67      1.00      0.80         4\n",
      "         210       1.00      1.00      1.00         5\n",
      "         212       1.00      1.00      1.00      5934\n",
      "         213       1.00      1.00      1.00         4\n",
      "         214       1.00      1.00      1.00         3\n",
      "         215       0.86      1.00      0.92         6\n",
      "         216       1.00      1.00      1.00       340\n",
      "         217       1.00      1.00      1.00    149656\n",
      "         218       1.00      1.00      1.00        10\n",
      "         219       1.00      1.00      1.00      5129\n",
      "         220       1.00      1.00      1.00       146\n",
      "         221       1.00      1.00      1.00       141\n",
      "         222       1.00      1.00      1.00        25\n",
      "         223       1.00      1.00      1.00       209\n",
      "         224       1.00      1.00      1.00        88\n",
      "         225       1.00      1.00      1.00       179\n",
      "         226       1.00      1.00      1.00    247005\n",
      "         227       1.00      1.00      1.00         3\n",
      "         228       1.00      1.00      1.00       328\n",
      "         229       1.00      1.00      1.00      5148\n",
      "         230       1.00      1.00      1.00        22\n",
      "         231       1.00      1.00      1.00        68\n",
      "         232       1.00      1.00      1.00      8407\n",
      "         233       1.00      1.00      1.00       191\n",
      "         234       1.00      1.00      1.00     66958\n",
      "         235       1.00      1.00      1.00     14578\n",
      "         236       1.00      1.00      1.00      1207\n",
      "         238       1.00      1.00      1.00       299\n",
      "         239       1.00      1.00      1.00         5\n",
      "         240       1.00      1.00      1.00       230\n",
      "         241       1.00      1.00      1.00         7\n",
      "         242       1.00      1.00      1.00        29\n",
      "         244       1.00      0.96      0.98        45\n",
      "         245       1.00      1.00      1.00        44\n",
      "         246       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00   1183010\n",
      "   macro avg       0.98      0.99      0.98   1183010\n",
      "weighted avg       1.00      1.00      1.00   1183010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56096c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9968900938469525\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# precision = precision_score(y_test, y_pred_class) # y_pred_class y_val_pred\n",
    "# recall = recall_score(y_test, y_pred_class)\n",
    "# f1 = f1_score(y_test, y_pred_class)\n",
    "\n",
    "# # Calculate the probabilities for the ROC AUC score\n",
    "# roc_auc = roc_auc_score(y_test, y_pred_class)\n",
    "\n",
    "# print(f\"Precision: {precision:.3f}\")\n",
    "# print(f\"Recall: {recall:.3f}\")\n",
    "# print(f\"F1-score: {f1:.3f}\")\n",
    "# print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Binarize the output\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_test)\n",
    "y_test_pred_bin = lb.transform(y_pred_class)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_bin, y_test_pred_bin, average='macro')\n",
    "print(f\"ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d278c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkIAAAHFCAYAAAAe+pb9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB3fklEQVR4nO3dd1gUZ9cG8HtpSwcFBbEgIgr2XjDWWLHEFht2jRpjjGA0tmiMUfOqsUbUoGADNVExaiwhsXdBrGBsKDZEQQGRuvt8f/gxyQoqiwtDuX/XxaVzpp2ZYXcOzzwzoxBCCBAREREVQ3pyJ0BEREQkFxZCREREVGyxECIiIqJii4UQERERFVsshIiIiKjYYiFERERExRYLISIiIiq2WAgRERFRscVCiIiIiIotFkJU4K1fvx4KhUL6MTAwQJkyZdCvXz/cvHlT7vQAABUrVsTQoUPlTiOLpKQk/Pjjj6hbty7Mzc1hZmaGOnXqYN68eUhKSpI7vRybN28edu3alSV+5MgRKBQKHDlyJN9zynTnzh2MGzcOVapUgYmJCUxNTVG9enXMmDEDDx8+lKZr1aoVatSoIVueHyIwMBBLly7Ns+Xn5vNz6tQpfPfdd3jx4kWWca1atUKrVq10khsVfQq+YoMKuvXr12PYsGHw9/eHq6srUlJScPLkScydOxcWFha4fv06SpQoIWuOYWFhsLS0hLOzs6x5/NeTJ0/Qtm1b3L59G+PHj8fHH38MADh06BCWLVsGZ2dn/PXXX7Czs5M50/czNzdH7969sX79eo14QkICwsPDUa1aNVhaWuZ7Xnv37kW/fv1ga2uLcePGoW7dulAoFLhy5Qr8/Pygp6eHsLAwAK9Pzs+ePcPVq1fzPc8P1aVLF1y9ehV3797Nk+Xn5vOzaNEiTJo0CZGRkahYsaLGuPDwcABAtWrVdJkmFVEGcidAlFM1atRAgwYNALw+qahUKsyaNQu7du3CsGHDZM2tbt26+b5OlUqFjIwMKJXKbMcPHjwY169fx+HDh/HRRx9J8Xbt2qFz585o3bo1hgwZggMHDuRXygDen7c2LC0t0aRJEx1kpb3IyEj069cPVapUweHDh2FlZSWNa9OmDcaPH4+goKB8zUkIgZSUFJiYmOTrenMrOTkZJiYmOv/8sAAibfDSGBVamUXRkydPNOIhISHo1q0bSpYsCWNjY9StWxe//vprlvkfPnyIUaNGoXz58jAyMoKDgwN69+6tsbyEhAR8/fXXcHJygpGREcqWLYsJEyZkuaz036b9p0+fwsjICN9++22WdV6/fh0KhQLLly+XYtHR0Rg9ejTKlSsHIyMjODk5Yfbs2cjIyJCmuXv3LhQKBRYsWIAffvgBTk5OUCqVOHz4cLb7JiQkBH/++SdGjBihUQRl+uijjzB8+HAcPHgQoaGhUlyhUGDcuHFYs2YNqlSpAqVSiWrVqmHr1q1ZlvGheaekpGDixImoU6cOrKysULJkSTRt2hS///67xnoUCgWSkpKwYcMG6fJo5mWP7C6NDR06FObm5rh16xY8PDxgbm6O8uXLY+LEiUhNTdVY9oMHD9C7d29YWFjA2toanp6eOH/+PBQKRZbWpzctXrwYSUlJ8PHx0SiC/pt3z549s8TPnz+P5s2bw9TUFJUqVcKPP/4ItVotjc/pfslcx7hx47B69Wq4ublBqVRiw4YNAIDZs2ejcePGKFmyJCwtLVGvXj2sW7cO2V0ECAwMRNOmTWFubg5zc3PUqVMH69atA/D6j44//vgD9+7d07hEnSktLQ0//PADXF1doVQqUapUKQwbNgxPnz7VWEfFihXRpUsX7Ny5E3Xr1oWxsTFmz54tjfvvpTG1Wo0ffvgBVatWhYmJCaytrVGrVi0sW7YMAPDdd99h0qRJAAAnJycpp8zfg+wujaWmpuL777+Hm5sbjI2NYWNjg9atW+PUqVNZ9gcVL2wRokIrMjISAFClShUpdvjwYXTs2BGNGzfG6tWrYWVlha1bt6Jv37549eqV9GX78OFDNGzYEOnp6Zg2bRpq1aqF2NhYHDx4EM+fP4ednR1evXqFli1b4sGDB9I0165dw8yZM3HlyhX89ddfGieETKVKlUKXLl2wYcMGzJ49G3p6//694e/vDyMjI3h6egJ4XUw0atQIenp6mDlzJpydnXH69Gn88MMPuHv3Lvz9/TWWvXz5clSpUgWLFi2CpaUlXFxcst03wcHBAIDu3bu/df91794dv/zyC4KDg1G/fn0pvnv3bhw+fBjff/89zMzM4OPjg/79+8PAwAC9e/fWWd6pqamIi4vD119/jbJlyyItLQ1//fUXevbsCX9/fwwePBgAcPr0abRp0watW7eWisv3XQZLT09Ht27dMGLECEycOBHHjh3DnDlzYGVlhZkzZwJ43X+qdevWiIuLw//+9z9UrlwZBw4cQN++fd+57Ex//vkn7OzstGqRio6OhqenJyZOnIhZs2YhKCgIU6dOhYODg7S9Od0vmXbt2oXjx49j5syZsLe3R+nSpQG8LkJHjx6NChUqAADOnDmDL7/8Eg8fPpT2AQDMnDkTc+bMQc+ePTFx4kRYWVnh6tWruHfvHgDAx8cHo0aNwu3bt7O0cKnVanzyySc4fvw4Jk+eDHd3d9y7dw+zZs1Cq1atEBISotE6deHCBURERGDGjBlwcnKCmZlZtvtpwYIF+O677zBjxgy0aNEC6enpuH79utQfaOTIkYiLi8OKFSuwc+dOlClTBsDbW4IyMjLQqVMnHD9+HBMmTECbNm2QkZGBM2fOICoqCu7u7jk6flRECaICzt/fXwAQZ86cEenp6SIxMVEcOHBA2NvbixYtWoj09HRpWldXV1G3bl2NmBBCdOnSRZQpU0aoVCohhBDDhw8XhoaGIjw8/K3rnT9/vtDT0xPnz5/XiG/fvl0AEPv27ZNijo6OYsiQIdLw7t27BQDx559/SrGMjAzh4OAgevXqJcVGjx4tzM3Nxb179zTWsWjRIgFAXLt2TQghRGRkpAAgnJ2dRVpa2vt2mRgzZowAIK5fv/7WaSIiIgQA8fnnn0sxAMLExERER0dr5O3q6ioqV66cp3lnZGSI9PR0MWLECFG3bl2NcWZmZhr7N9Phw4cFAHH48GEpNmTIEAFA/PrrrxrTenh4iKpVq0rDK1euFADE/v37NaYbPXq0ACD8/f3fma+xsbFo0qTJO6f5r5YtWwoA4uzZsxrxatWqiQ4dOrx1vnftFwDCyspKxMXFvXPdKpVKpKeni++//17Y2NgItVothBDizp07Ql9fX3h6er5z/s6dOwtHR8cs8S1btggAYseOHRrx8+fPCwDCx8dHijk6Ogp9fX3xzz//ZFnOm5+fLl26iDp16rwzp4ULFwoAIjIyMsu4li1bipYtW0rDGzduFACEr6/vO5dJxRMvjVGh0aRJExgaGsLCwgIdO3ZEiRIl8Pvvv8PA4HXD5q1bt3D9+nWptSUjI0P68fDwwOPHj/HPP/8AAPbv34/WrVvDzc3trevbu3cvatSogTp16mgsq0OHDu+9U6lTp06wt7fXaBk5ePAgHj16hOHDh2uso3Xr1nBwcNBYR6dOnQAAR48e1Vhut27dYGhoqN2Oewvx/5dI3mzV+vjjjzU6UOvr66Nv3764desWHjx4oNO8f/vtNzRr1gzm5uYwMDCAoaEh1q1bh4iIiA/aNoVCga5du2rEatWqJbVyZOaY+bv0X/379/+gdb+Lvb09GjVq9M68AO32S5s2bbK9WeDQoUNo27YtrKysoK+vD0NDQ8ycOROxsbGIiYkB8LrlUKVS4YsvvsjV9uzduxfW1tbo2rWrxu9BnTp1YG9vn+UzUqtWLY0W3Ldp1KgRLl26hLFjx+LgwYNISEjIVX6Z9u/fD2NjY43PHlEmFkJUaGzcuBHnz5/HoUOHMHr0aERERGictDL79nz99dcwNDTU+Bk7diwA4NmzZwBe9+MpV67cO9f35MkTXL58OcuyLCwsIISQlpUdAwMDDBo0CEFBQVJz/vr161GmTBl06NBBYx179uzJso7q1atr5Jsp8xLA+2ReDsm8fJidzDuAypcvrxG3t7fPMm1mLDY2Vmd579y5E3369EHZsmWxefNmnD59GufPn8fw4cORkpKSo+18G1NTUxgbG2vElEqlxnJjY2OzvWMup3fRVahQ4Z37Nzs2NjZZYkqlEsnJydKwtvslu3177tw5tG/fHgDg6+uLkydP4vz585g+fToASOvL7Mfzvs/C2zx58gQvXryAkZFRlt+F6OjoXP/+Tp06FYsWLcKZM2fQqVMn2NjY4OOPP0ZISEiu8nz69CkcHBw0LlMTZWIfISo03NzcpA7SrVu3hkqlwtq1a7F9+3b07t0btra2AF5/iWbXSRUAqlatCuB1P57M1o23sbW1hYmJCfz8/N46/l2GDRuGhQsXSn2Udu/ejQkTJkBfX19jGbVq1cLcuXOzXYaDg4PGcHZ9krLTrl07TJs2Dbt27crS4pEp87k87dq104hHR0dnmTYzlnki10XemzdvhpOTE7Zt26Yx/s0OzXnFxsYG586dyxLPbvuz06FDB6xYsQJnzpzR6Z1r2u6X7Pbt1q1bYWhoiL1792oUhG8+i6lUqVIAXncaf7MgzglbW1vY2Ni89c5DCwuL9+aaHQMDA3h7e8Pb2xsvXrzAX3/9hWnTpqFDhw64f/8+TE1NtcqzVKlSOHHiBNRqNYshyoKFEBVaCxYswI4dOzBz5kz07NkTVatWhYuLCy5duoR58+a9c95OnTph06ZN+Oeff6Ti6E1dunTBvHnzYGNjAycnJ63zc3NzQ+PGjeHv7w+VSoXU1NQst/l36dIF+/btg7Ozs06fhdSgQQO0b98e69atw6BBg9CsWTON8SdOnICfnx86duyo0VEaAP7++288efJEahlRqVTYtm0bnJ2dpZYDXeStUChgZGSkcXKMjo7O9u6oN1tNdKFly5b49ddfsX//fumSHoBs75DLjpeXF/z8/DB27Ngst88Dry897tq1Cz169NAqL232y7uWYWBgoFF0JycnY9OmTRrTtW/fHvr6+li1ahWaNm361uW9bf936dIFW7duhUqlQuPGjXOcnzasra3Ru3dvPHz4EBMmTMDdu3dRrVo16fELOfm96NSpE7Zs2YL169fz8hhlwUKICq0SJUpg6tSpmDx5MgIDAzFw4ECsWbMGnTp1QocOHTB06FCULVsWcXFxiIiIwIULF/Dbb78BAL7//nvs378fLVq0wLRp01CzZk28ePECBw4cgLe3N1xdXTFhwgTs2LEDLVq0gJeXF2rVqgW1Wo2oqCj8+eefmDhx4nu//IcPH47Ro0fj0aNHcHd3z1J0ff/99wgODoa7uzvGjx+PqlWrIiUlBXfv3sW+ffuwevXqXF+22LhxI9q2bYv27dtn+0BFV1fXbG8Rt7W1RZs2bfDtt99Kd41dv35do0DQRd6Zt1KPHTsWvXv3xv379zFnzhyUKVMmyxPDa9asiSNHjmDPnj0oU6YMLCws3lrA5tSQIUOwZMkSDBw4ED/88AMqV66M/fv34+DBgwDw3pYDJycnqbWvTp060gMVgdcP9PPz84MQQutCSJv98jadO3fG4sWLMWDAAIwaNQqxsbFYtGhRlmc3VaxYEdOmTcOcOXOQnJyM/v37w8rKCuHh4Xj27Jl0e3vNmjWxc+dOrFq1CvXr14eenh4aNGiAfv36ISAgAB4eHvjqq6/QqFEjGBoa4sGDBzh8+DA++eQTrbcfALp27So9N6xUqVK4d+8eli5dCkdHR+lOyZo1awIAli1bhiFDhsDQ0BBVq1bN0goFvO735e/vjzFjxuCff/5B69atoVarcfbsWbi5uaFfv35a50hFiLx9tYneL/OusTfv3hJCiOTkZFGhQgXh4uIiMjIyhBBCXLp0SfTp00eULl1aGBoaCnt7e9GmTRuxevVqjXnv378vhg8fLuzt7YWhoaFwcHAQffr0EU+ePJGmefnypZgxY4aoWrWqMDIyElZWVqJmzZrCy8tL486qN+96yRQfHy9MTEzeecfK06dPxfjx44WTk5MwNDQUJUuWFPXr1xfTp08XL1++FEL8e/fVwoULtdp3L1++FPPmzRN16tQRpqamwtTUVNSqVUv88MMP0rL/C4D44osvhI+Pj3B2dhaGhobC1dVVBAQE5EneP/74o6hYsaJQKpXCzc1N+Pr6ilmzZok3v5ouXrwomjVrJkxNTQUA6Y6gt901ZmZmlmVd2S03KipK9OzZU5ibmwsLCwvRq1cvsW/fPgFA/P777+/ct5lu374txo4dKypXriyUSqUwMTER1apVE97e3hp3NLVs2VJUr149y/xDhgzJckdWTvdL5vHKjp+fn6hatapQKpWiUqVKYv78+WLdunXZ3mm1ceNG0bBhQ2FsbCzMzc1F3bp1Ne6ai4uLE7179xbW1tZCoVBo5JGeni4WLVokateuLc3v6uoqRo8eLW7evClN5+joKDp37pxtrm9+fn766Sfh7u4ubG1thZGRkahQoYIYMWKEuHv3rsZ8U6dOFQ4ODkJPT0/j9+DNu8aEeP1dMXPmTOHi4iKMjIyEjY2NaNOmjTh16lS2OVHxwVdsEJFEoVDgiy++wM8//yx3KrKZN28eZsyYgaioqFy3xhFR4cFLY0RUbGUWfK6urkhPT8ehQ4ewfPlyDBw4kEUQUTHBQoiIii1TU1MsWbIEd+/eRWpqKipUqIBvvvkGM2bMkDs1IsonvDRGRERExRYfqEBERETFFgshIiIiKrZYCBEREVGxVew6S6vVajx69AgWFhY5ftw7ERERyUsIgcTERJ2/N67YFUKPHj3K1Tt1iIiISH7379/X6eMtil0hlPn49fv378PS0lLmbIiIiCgnEhISUL58+Wxfo/Ihil0hlHk5zNLSkoUQERFRIaPrbi3sLE1ERETFFgshIiIiKrZYCBEREVGxxUKIiIiIii0WQkRERFRssRAiIiKiYouFEBERERVbLISIiIio2GIhRERERMUWCyEiIiIqtmQthI4dO4auXbvCwcEBCoUCu3bteu88R48eRf369WFsbIxKlSph9erVeZ8oERERFUmyFkJJSUmoXbs2fv755xxNHxkZCQ8PDzRv3hxhYWGYNm0axo8fjx07duRxpkRERFQUyfrS1U6dOqFTp045nn716tWoUKECli5dCgBwc3NDSEgIFi1ahF69euVRlkRERFRUFaq3z58+fRrt27fXiHXo0AHr1q1Deno6DA0NZcqMiIgkQgAQmv8KtWbsfePfnO6d49XZr1P69wPHZ6472/H/mfd947Od7h3j37bN/503t+PfuU1ajNd2m3I7Xghcjcj1b+Q7FapCKDo6GnZ2dhoxOzs7ZGRk4NmzZyhTpkyWeVJTU5GamioNJyQk5HmeJKP3fnl8yBeeFl+IOvzwf/AXnq6+MHN6AtPqBPeObf7gE5wO9omutlmrfVLI9xmRjsUnKzEuyAObL1TNk+UXqkIIABQKhcawECLbeKb58+dj9uzZWUecXwCYKvHeL7wPPcHl9gsvR188BeSvnIKyz4iIqEg5GVkeAwN74u7zEgBS8mQdhaoQsre3R3R0tEYsJiYGBgYGsLGxyXaeqVOnwtvbWxpOSEhA+fLlgTNzAeM8TZeIKBcUgELxln/18m78m+N0Ol7v7Tl96Pi83Cf5vc90tk8UAD5wn70t5/eNf+c2aTc+NVWNfvV34cHzVwAACwtDJCa+7/OjvUJVCDVt2hR79uzRiP35559o0KDBW/sHKZVKKJXK/EivCHjPF1ce/KJr9+H90PEf8OHOxw9/vu+zzGV/6D7J6QkwR9v0gePfuU1ajNd2m953gnrfeIUiuw8mUbGkBLDOryc6dNiMZs3KY9WqtqhVa47O1yNrIfTy5UvcunVLGo6MjMTFixdRsmRJVKhQAVOnTsXDhw+xceNGAMCYMWPw888/w9vbG5999hlOnz6NdevWYcuWLblLoMfe93zhve8L8X0nsEJS0fPLl4iIZCaEQEpKBkxM/m3YaN/eGQcPDkSbNk549eplnqxX1kIoJCQErVu3loYzL2ENGTIE69evx+PHjxEVFSWNd3Jywr59++Dl5YWVK1fCwcEBy5cvz92t86XrAJU6f+gmEBER0QeKi0vGmDF7kZycgd27++G//X7bt3fO03UrRGZv42IiISEBVlZWiP+lLiw/uyB3OkRERMXa4cORGDQoCA8fvu4A5OPjgc8/b5hlOun8HR8PS0tLna2f7xojIiKifJeWpsLkycH4+OONUhFUooQx7O3N8zWPQtVZmoiIiAq/69efYcCAHQgL+/dO8DZtnLBhQ3eUK6e71p6cYCFERERE+UIIgTVrQuHtfRDJyRkAAENDPcyf/zG8vJpCTy//b95hIURERER5LjU1A59++hv27LkhxdzcbBEQ0BN162Z9M0R+YR8hIiIiynNKpQEsLP59rt/YsQ0QEjJK1iIIYIsQERER5ZOVKz1w82YsZs5siS5dqsidDgAWQkRERJQHLl9+gkePEtGxY2UpZm1tjLNnR+Jt7weVAy+NERERkc6o1QJLlpxGw4a+GDBgBx48SNAYX5CKIICFEBEREenI6xagzfD2/hNpaSo8f56CefOOy53WO/HSGBEREX2wXbuuY+TI3YiNTZZiEyc2xdy5bWTM6v1YCBEREVGuJSWlwcvrIHx9/31tVZky5ti4sQfatq0kY2Y5w0KIiIiIciUk5BE8PXfixo1YKdajhyt8fbvCxsZUxsxyjoUQERERaS0lJQPdum3B48cvAQCmpoZYvrwjhg+vW+A6RL8LO0sTERGR1oyNDeDj0xkA0LChAy5eHI0RI+oVqiIIYIsQERER5VBamgpGRvrScPfurggK6ovOnV1gaKj/jjkLLrYIERER0TvFx6dg0KAgDBy4E0IIjXHdu7sW2iIIYIsQERERvcPJk1EYODAId+++AAB07nwJQ4bUkTUnXWKLEBEREWWRnq7CzJmH0aLFeqkIsrRUwti4aLWhFK2tISIiog9261YcBg7cibNnH0qxZs3KY/PmnqhY0Vq+xPIACyEiIiICAAghsH79RXz55X4kJaUDAPT1Ffjuu1aYMuUjGBgUvQtJLISIiIgIKSkZGDQoCNu3h0sxZ+cSCAjoicaNy8mYWd5iIURERERQKvWRnq6ShkeMqIulSzvC3NxIxqzyXtFr4yIiIiKtKRQKrF3bDdWrl8L27Z9i7dpuRb4IAtgiREREVCxdv/4MT568RMuWFaWYra0pLl/+HHp6hevp0B+CLUJERETFiBACq1eHoF69NejTZzuePHmpMb44FUEACyEiIqJiIyYmCZ98shWff/4HkpMzEBOThDlzjsmdlqx4aYyIiKgY2L//JoYN+x1PniRJsS++aIgFC9rJmJX8WAgREREVYcnJ6fjmm7+wYsU5KVa6tBn8/Lqhc+cqMmZWMLAQIiIiKqIuXYqGp+dOXLv2VIp5eLjAz68b7OzMZcys4GAhREREVAQlJ6ejffvNiIl5fSnM2NgAixa1w9ixDaFQFK8O0e/CztJERERFkImJIZYs6QAAqF3bDqGho/DFF41YBL2BLUJERERFhEqlhr7+v20cAwbUhBACvXtXg1LJU3522CJERERUyCUlpWHUqD0YOXJPlnGenrVYBL0D9wwREVEhFhLyCJ6eO3HjRiwAwMOjMj79tLrMWRUebBEiIiIqhFQqNebPP46mTddJRZCpqSFSU1XvmZP+q/i2CDX9Tu4MiIiIciUqKh6DBgXh2LF7UqxBAwcEBPRElSo2MmZW+BTfQqhCK7kzICIi0trWrVcxZsxexMenAgAUCmDatOaYNaslDA31Zc6u8Cm+hRAREVEhkpycjtGj92LTpstSrEIFK2ze3APNmzvKmFnhxkKIiIioEFAqDTTeEzZgQE2sXOkBa2tjGbMq/NhZmoiIqBDQ01Ng/fpP4OxcAps390BAQE8WQTrAFiEiIqIC6NatOMTGvkLjxuWkWJkyFrh+fRwMDNiOoSvck0RERAWIEAL+/mGoU2c1evX6FXFxyRrjWQTpFvcmERFRAREXl4w+fbZj+PDdSEpKx8OHiZg9+4jcaRVpvDRGRERUABw+HIlBg4Lw8GGiFBsxoi7mzv1YxqyKPhZCREREMkpLU2HGjENYtOgUhHgdK1HCGL6+XdGrVzV5kysGWAgRERHJ5Pr1ZxgwYAfCwqKlWJs2TtiwoTvKlbOUMbPig4UQERGRDF69SkeLFv54+vQVAMDQUA/z538ML6+m0NNTyJxd8cHO0kRERDIwNTXE3LltAABubrY4d+4zTJzoziIon7FFiIiIKJ8IIaBQ/FvojBxZD0IAAwfWgqmpoYyZFV8shIiIiPJYcnI6vvnmLwghsGKFhxRXKBQYNaq+jJkRCyEiIqI8dOlSNDw9d+LatacAgI4dK6Nz5yoyZ0WZ2EeIiIgoD6jVAkuWnEajRmulIsjY2EDqHE0FA1uEiIiIdOzRo0QMHboLwcF3pFjt2nYIDOyFatVKyZgZvYmFEBERkQ4FBUXgs8/2IDb233eETZzYFHPntoFSydNuQcMjQkREpAMpKRkYP34/fH0vSDEHBwts2NAdbdtWkjEzehcWQkRERDpgaKiH69efScM9erjC17crbGxMZcyK3oedpYmIiHRAX18Pmzb1QNmyFli7tit27OjDIqgQYIsQERFRLty79wLPn6egTh17KeboaI3bt8ezL1AhwhYhIiIiLW3ZcgW1a69Gz57bkJCQqjGORVDhwkKIiIgoh+LjUzBoUBAGDNiJ+PhUREa+wOzZR+ROiz6A7IWQj48PnJycYGxsjPr16+P48ePvnD4gIAC1a9eGqakpypQpg2HDhiE2NjafsiUiouLq5Mko1KmzBps3X5ZiAwbUxMyZLWXMij6UrIXQtm3bMGHCBEyfPh1hYWFo3rw5OnXqhKioqGynP3HiBAYPHowRI0bg2rVr+O2333D+/HmMHDkynzMnIqLiIj1dhZkzD6NFi/W4e/cFAMDSUonNm3sgIKAnrKyM5U2QPohCCCHkWnnjxo1Rr149rFq1Soq5ubmhe/fumD9/fpbpFy1ahFWrVuH27dtSbMWKFViwYAHu37+fo3UmJCTAysoK8fHxsLS0/PCNICKiIuv27Th4eu7E2bMPpdhHH1XApk09ULGitXyJFUN5df6WrUUoLS0NoaGhaN++vUa8ffv2OHXqVLbzuLu748GDB9i3bx+EEHjy5Am2b9+Ozp07v3U9qampSEhI0PghIiJ6n6SkNDRpsk4qgvT1Ffjhh9Y4cmQIi6AiRLZC6NmzZ1CpVLCzs9OI29nZITo6Ott53N3dERAQgL59+8LIyAj29vawtrbGihUr3rqe+fPnw8rKSvopX768TreDiIiKJjMzI8yY0RwA4OxcAqdOjcD06S2gry9791rSIdmPpkKh0BgWQmSJZQoPD8f48eMxc+ZMhIaG4sCBA4iMjMSYMWPeuvypU6ciPj5e+snpJTQiIip+3uwt8uWXjbF4cXtcvDgGjRqVlSkrykuyPezA1tYW+vr6WVp/YmJisrQSZZo/fz6aNWuGSZMmAQBq1aoFMzMzNG/eHD/88APKlCmTZR6lUgmlUqn7DSAioiIjLU2FGTMOQU9PgR9/bCvF9fQU8PJqKmNmlNdkaxEyMjJC/fr1ERwcrBEPDg6Gu7t7tvO8evUKenqaKevr6wPIWsUTERHlRETEUzRpshYLF57CggUncfhwpNwpUT6S9dKYt7c31q5dCz8/P0RERMDLywtRUVHSpa6pU6di8ODB0vRdu3bFzp07sWrVKty5cwcnT57E+PHj0ahRIzg4OMi1GUREVAgJIbBq1XnUr/8LwsJeX50wMNDD7dvPZc6M8pOszwHv27cvYmNj8f333+Px48eoUaMG9u3bB0dHRwDA48ePNZ4pNHToUCQmJuLnn3/GxIkTYW1tjTZt2uB///ufXJtARESFUExMEkaM2I29e29IMTc3WwQG9tJ4dxgVfbI+R0gOfI4QEVHxtn//TQwd+jtiYpKk2NixDbBwYXuYmhrKmBm9S16dv/lmOCIiKhZSUjIweXIwVqw4J8VKlTKFn98n6NKlioyZkZxYCBERUbGgr6/AmTMPpGEPDxf4+XWDnZ25jFmR3GR/jhAREVF+MDTUR0BAT9jamuLnnzth797+LIKILUJERFQ0PXqUiPj4FLi5lZJiLi42uHv3K5iZGcmYGRUkbBEiIqIiJygoArVqrUKvXr/i1at0jXEsgui/WAgREVGRkZSUhlGj9qBnz18RG5uMiIhn+P77o3KnRQUYL40REVGREBLyCJ6eO3HjRqwU69HDFZMmZf+2AiKAhRARERVyKpUaCxacxMyZR5CRoQYAmJoaYvnyjhg+vO5bX+RNBLAQIiKiQiwqKh6DBgXh2LF7UqxhQwcEBPSEi4uNjJlRYcFCiIiICqXExFQ0aPALnj59BQBQKIBp05pj1qyWMDTUlzk7KizYWZqIiAolCwslJkxoAgCoUMEKR48OxQ8/tGERRFphixARERVa33zTDGq1wLhxjWBtbSx3OlQIsRAiIqICLyNDjTlzjsLAQA/ffttSiuvr62HGjBYyZkaFHQshIiIq0G7fjoOn506cPfsQenoKtG1bCU2blpc7LSoi2EeIiIgKJCEE1q+/iDp11uDs2YcAXneIvnTpicyZUVHCFiEiIipw4uKSMXr0XmzfHi7FnJ1LICCgJxo3LidjZlTUsBAiIqIC5fDhSAwaFISHDxOl2IgRdbF0aUeYm/M9YaRbLISIiKhASEtT4dtvD2HhwlMQ4nWsRAlj+Pp2Ra9e1eRNjoosFkJERFQgqNUC+/ffkoqgNm2csGFDd5QrZylvYlSksbM0EREVCMbGBggM7AVLSyUWLWqH4OBBLIIoz7FFiIiIZBETk4TExFQ4O5eUYjVqlMa9exP4cETKN2wRIiKifLd//03UrLkKvXv/htTUDI1xLIIoP7EQIiKifJOcnI7x4/fDwyMQMTFJuHgxGnPnHpc7LSrGeGmMiIjyxaVL0fD03Ilr155KMQ8PF3zxRUMZs6LijoUQERHlKbVaYNmyM5gy5W+kpakAvO4YvWhRO4wd2xAKhULmDKk4YyFERER55tGjRAwZsgt//XVHitWubYfAwF6oVq2UjJkRvcZCiIiI8kR8fArq1FmNp09fSbGJE5ti7tw2UCp5+qGCgZ2liYgoT1hZGWPUqPoAAAcHCwQHD8KiRe1ZBFGBwt9GIiLKM7NmtYRaLTBxYlPY2JjKnQ5RFrlqEcrIyMBff/2FNWvWIDHx9UvxHj16hJcvX+o0OSIiKhxUKjXmzz+OJUtOa8QNDfUxb97HLIKowNK6RejevXvo2LEjoqKikJqainbt2sHCwgILFixASkoKVq9enRd5EhFRARUVFY9Bg4Jw7Ng9GBrqoVWriqhbt4zcaRHliNYtQl999RUaNGiA58+fw8TERIr36NEDf//9t06TIyKigm3r1quoVWsVjh27BwDIyFDj1Kn7MmdFlHNatwidOHECJ0+ehJGRkUbc0dERDx8+1FliRERUcCUkpGLcuH3YtOmyFKtQwQqbN/dA8+aOMmZGpB2tCyG1Wg2VSpUl/uDBA1hYWOgkKSIiKrhOnozCwIFBuHv3hRQbMKAmVq704HvCqNDR+tJYu3btsHTpUmlYoVDg5cuXmDVrFjw8PHSZGxERFSDp6SrMnHkYLVqsl4ogS0slNm/ugYCAniyCqFDSukVoyZIlaN26NapVq4aUlBQMGDAAN2/ehK2tLbZs2ZIXORIRUQGQlqbCtm3XoFYLAMBHH1XApk09ULGitbyJEX0AhRBCaDtTcnIytm7ditDQUKjVatSrVw+enp4anacLqoSEBFhZWSE+Ph6WlpZyp0NEVKiEhDxCixb+mD69OaZM+Qj6+nwuL+WPvDp/a10IHTt2DO7u7jAw0GxMysjIwKlTp9CiRQudJZcXWAgREeVMXFwykpLSUL68lUY8JiYJpUubyZQVFVd5df7WupRv3bo14uLissTj4+PRunVrnSRFRETyOnw4ErVqrUKfPtuRkaHWGMciiIoSrQshIQQUCkWWeGxsLMzM+OEgIirM0tJUmDw5GB9/vBEPHybizJkH+N//TsidFlGeyXFn6Z49ewJ4fZfY0KFDoVQqpXEqlQqXL1+Gu7u77jMkIqJ8ERHxFJ6eOxEWFi3F2rRxwpAhdeRLiiiP5bgQsrJ6fY1YCAELCwuNjtFGRkZo0qQJPvvsM91nSEREeUoIgTVrQuHtfRDJyRkAAENDPcyb9zG8vZtCTy/rVQCioiLHhZC/vz8AoGLFivj66695GYyIqAiIiUnCyJG7sWfPDSnm5maLgICefF8YFQu5un2+MONdY0REr714kQI3t5WIjn4pxcaObYCFC9vD1NRQxsyIssqr87fWD1QEgO3bt+PXX39FVFQU0tLSNMZduHBBJ4kREVHesrY2Rr9+1bF06VmUKmUKP79P0KVLFbnTIspXWt81tnz5cgwbNgylS5dGWFgYGjVqBBsbG9y5cwedOnXKixyJiCiPzJ/fFuPHN8KVK5+zCKJiSetLY66urpg1axb69+8PCwsLXLp0CZUqVcLMmTMRFxeHn3/+Oa9y1QleGiOi4kitFli27AzMzIwwalR9udMh0lqBeaBiVFSUdJu8iYkJEhMTAQCDBg3iu8aIiAqgR48S0bHjZnh7/4mvvjqAiIincqdEVGBoXQjZ29sjNjYWAODo6IgzZ84AACIjI1HM+l0TERV4QUERqFVrFYKD7wAAUlIypP8TUS46S7dp0wZ79uxBvXr1MGLECHh5eWH79u0ICQmRHrpIRETySkpKg5fXQfj6/nsDi4ODBTZs6I62bSvJmBlRwaJ1HyG1Wg21Wi29dPXXX3/FiRMnULlyZYwZMwZGRkZ5kqiusI8QERV1ISGP4Om5EzduxEqxHj1c4evbFTY2pjJmRpR7Bebt8+/y8OFDlC1bVleLyxMshIioqFKp1Fiw4CRmzjwivSjV1NQQy5d3xPDhdbN9TyRRYVFgOktnJzo6Gl9++SUqV66si8UREVEuJCWlY82aUKkIatjQARcvjsaIEfVYBBG9RY4LoRcvXsDT0xOlSpWCg4MDli9fDrVajZkzZ6JSpUo4c+YM/Pz88jJXIiJ6B0tLJTZt6gFDQz1Mn94cJ08Oh4uLjdxpERVoOe4sPW3aNBw7dgxDhgzBgQMH4OXlhQMHDiAlJQX79+9Hy5Yt8zJPIiJ6Q0JCKl69Soe9vbkUa97cEbdvj0f58lYyZkZUeOS4ReiPP/6Av78/Fi1ahN27d0MIgSpVquDQoUMsgoiI8tnJk1GoXXs1BgzYAbVas6sniyCinMtxIfTo0SNUq1YNAFCpUiUYGxtj5MiReZYYERFllZ6uwsyZh9GixXrcvfsChw/fxZIlp+VOi6jQyvGlMbVaDUPDf99GrK+vDzMzszxJioiIsrp1Kw4DB+7E2bMPpdhHH1VAr17VZMyKqHDLcSEkhMDQoUOhVCoBACkpKRgzZkyWYmjnzp26zZCIqJgTQmD9+ov48sv9SEpKBwDo6yswe3YrTJnyEfT1dXIDMFGxlONPz5AhQ1C6dGlYWVnBysoKAwcOhIODgzSc+aMtHx8fODk5wdjYGPXr18fx48ffOX1qaiqmT58OR0dHKJVKODs78241Iiqy4uKS0afPdgwfvlsqgpydS+DUqRGYPr0FiyCiD5TjFiF/f3+dr3zbtm2YMGECfHx80KxZM6xZswadOnVCeHg4KlSokO08ffr0wZMnT7Bu3TpUrlwZMTExyMjI0HluRERye/48GbVrr8aDBwlSbMSIuli6tCPMzQv2U/yJCgudPllaW40bN0a9evWwatUqKebm5obu3btj/vz5WaY/cOAA+vXrhzt37qBkyZK5WiefLE1Ehcno0Xvwyy8XUKKEMXx9u7I/EBVbBfrJ0rmRlpaG0NBQtG/fXiPevn17nDp1Ktt5du/ejQYNGmDBggUoW7YsqlSpgq+//hrJycn5kTIRUb5bvLgDRoyoi8uXP2cRRJQHtH77vK48e/YMKpUKdnZ2GnE7OztER0dnO8+dO3dw4sQJGBsbIygoCM+ePcPYsWMRFxf31n5CqampSE1NlYYTEhKynY6ISE5CCKxZEwpzcyMMHFhLipuZGWHt2m4yZkZUtMlWCGV68/03Qoi3vhNHrVZDoVAgICBA6pi9ePFi9O7dGytXroSJiUmWeebPn4/Zs2frPnEiIh2JiUnCyJG7sWfPDZibG6Fp03Jwds7d5X8i0o5sl8ZsbW2hr6+fpfUnJiYmSytRpjJlyqBs2bIad6e5ublBCIEHDx5kO8/UqVMRHx8v/dy/f193G0FE9IH277+JWrVWYc+eGwCAly/TsHfvDZmzIio+clUIbdq0Cc2aNYODgwPu3bsHAFi6dCl+//33HC/DyMgI9evXR3BwsEY8ODgY7u7u2c7TrFkzPHr0CC9fvpRiN27cgJ6eHsqVK5ftPEqlEpaWlho/RERyS05Ox/jx++HhEYgnT5IAAKVKmWLPnv746qsmMmdHVHxoXQitWrUK3t7e8PDwwIsXL6BSqQAA1tbWWLp0qVbL8vb2xtq1a+Hn54eIiAh4eXkhKioKY8aMAfC6NWfw4MHS9AMGDICNjQ2GDRuG8PBwHDt2DJMmTcLw4cOzvSxGRFQQXb78BA0b+mLFinNSzMPDBVeufI4uXarImBlR8aN1IbRixQr4+vpi+vTp0NfXl+INGjTAlStXtFpW3759sXTpUnz//feoU6cOjh07hn379sHR0REA8PjxY0RFRUnTm5ubIzg4GC9evECDBg3g6emJrl27Yvny5dpuBhFRvlOrBZYsOY2GDX1x7dpTAICxsQF+/rkT9u7tDzs78/csgYh0TevnCJmYmOD69etwdHSEhYUFLl26hEqVKuHmzZuoVatWgb+Vnc8RIiK5PH+ejOrVffD48evL+7Vq2SEwsCeqVy8tc2ZEBV+BeY6Qk5MTLl68mCW+f/9+6e30RESUVYkSJtiwoTv09BSYOLEpzp0bySKISGZa3z4/adIkfPHFF0hJSYEQAufOncOWLVswf/58rF27Ni9yJCIqlJKS0pCSkgEbG1Mp1q6dM/75ZxwqV+bt8UQFgdaF0LBhw5CRkYHJkyfj1atXGDBgAMqWLYtly5ahX79+eZEjEVGhExLyCJ6eO1G5ckns3dtf4/loLIKICo4PetfYs2fPoFarUbp04WnaZR8hIspLKpUaCxacxMyZR5CRoQYArFzpgbFjG8qcGVHhVmD6CM2ePRu3b98G8PqhiIWpCCIiyktRUfFo02Yjpk07JBVBDRs6oF27SjJnRkRvo3UhtGPHDlSpUgVNmjTBzz//jKdPn+ZFXkREhcrWrVdRq9YqHDv2+iGzenoKTJ/eHCdPDoeLi43M2RHR22hdCF2+fBmXL19GmzZtsHjxYpQtWxYeHh4IDAzEq1ev8iJHIqICKyEhFYMHB6F//x2Ij3/9gucKFaxw5MgQ/PBDGxga6r9nCUQkpw/qIwQAJ0+eRGBgIH777TekpKQU+Le7s48QEelKbOwrNGzoi8jIF1JswICaWLnSA9bWxvIlRlQEFZg+Qm8yMzODiYkJjIyMkJ6erouciIgKBRsbUzRrVgEAYGmpxObNPRAQ0JNFEFEhovXt8wAQGRmJwMBABAQE4MaNG2jRogW+++47fPrpp7rOj4ioQPv5505QqdSYN+9jVKxoLXc6RKQlrQuhpk2b4ty5c6hZsyaGDRsmPUeIiKgoE0Jgw4ZLsLRUomdPNyluZWWMwMBeMmZGRB9C60KodevWWLt2LapXr54X+RARFThxcckYPXovtm8Ph7W1MRo2dED58lZyp0VEOqB1H6F58+axCCKiYuPw4UjUqrUK27eHAwBevEiR/k9EhV+OWoS8vb0xZ84cmJmZwdvb+53TLl68WCeJERHJKS1NhRkzDmHRolPIvLe2RAlj+Pp2Ra9efME0UVGRo0IoLCxMuiMsLCwsTxMiIpLb9evPMGDADoSFRUuxNm2csGFDd5Qrx8duEBUlH/wcocKGzxEiorcRQmDNmlB4ex9EcnIGAMDQUA/z538ML6+m0NNTvGcJRJRXCsxzhIYPH47ExMQs8aSkJAwfPlwnSRERySEuLhnffntYKoLc3Gxx7txnmDjRnUUQURGldSG0YcMGJCcnZ4knJydj48aNOkmKiEgONjamWLu2KwBg7NgGCAkZhTp17GXOiojyUo5vn09ISIAQAkIIJCYmwtj43yenqlQq7Nu3j2+iJ6JCJTk5HWlpKlhZ/ft99sknrrh8eQxq1rSTMTMiyi85LoSsra2hUCigUChQpUqVLOMVCgVmz56t0+SIiPLK5ctPMGDADri5lcKvv/aGQvHvpS8WQUTFR44LocOHD0MIgTZt2mDHjh0oWbKkNM7IyAiOjo5wcHDIkySJiHRFrRZYtuwMpkz5G2lpKly79hQbNlzC0KF15E6NiGSQ40KoZcuWAF6/Z6xChQoafz0RERUGjx4lYujQXQgOviPFate2Q6NGfE0QUXGVo0Lo8uXLqFGjBvT09BAfH48rV668ddpatWrpLDkiIl0JCorAZ5/tQWzsvzd7TJzYFHPntoFSmav3TxNREZCjT3+dOnUQHR2N0qVLo06dOlAoFMju8UMKhQIqlUrnSRIR5VZSUhq8vA7C1/eCFHNwsMCGDd3Rtm0lGTMjooIgR4VQZGQkSpUqJf2fiKgwePo0CR995I8bN2KlWI8ervD17QobG1MZMyOigiJHhZCjo2O2/yciKshsbU1RvXop3LgRC1NTQyxf3hHDh9dlH0cikuTqgYp//PGHNDx58mRYW1vD3d0d9+7d02lyREQfQqFQwNe3K7p1q4qLF0djxIh6LIKISIPWhdC8efNgYmICADh9+jR+/vlnLFiwALa2tvDy8tJ5gkREObV161Xs339TI2ZjY4rff+8HFxcbmbIiooJM61sl7t+/j8qVKwMAdu3ahd69e2PUqFFo1qwZWrVqpev8iIjeKyEhFePG7cOmTZdRqpQprlz5HHZ25nKnRUSFgNYtQubm5oiNfd3x8M8//0Tbtm0BAMbGxtm+g4yIKC+dPBmF2rVXY9OmywCAp09fISDg7Y/4ICL6L61bhNq1a4eRI0eibt26uHHjBjp37gwAuHbtGipWrKjr/IiIspWersKcOccwd+5xqNWvH+dhaamEj48HPD35PDMiyhmtW4RWrlyJpk2b4unTp9ixYwdsbF5fdw8NDUX//v11niAR0Ztu3YpD8+b+mDPnmFQEffRRBVy6NIZFEBFpRSGyezJiEZaQkAArKyvEx8fD0tJS7nSISAtCCKxffxFffrkfSUnpAAB9fQVmz26FKVM+gr6+1n/bEVEhkVfn71w9V/7FixdYt24dIiIioFAo4ObmhhEjRsDKykpniRERvenp01fw8jooFUHOziUQENATjRuXkzkzIiqstP7zKSQkBM7OzliyZAni4uLw7NkzLFmyBM7Ozrhw4cL7F0BElEulS5th9eouAIARI+ri4sUxLIKI6INofWmsefPmqFy5Mnx9fWFg8LpBKSMjAyNHjsSdO3dw7NixPElUV3hpjKjwSEtTIT1dBTMzI434uXMP+cZ4omImr87fWhdCJiYmCAsLg6urq0Y8PDwcDRo0wKtXr3SWXF5gIURUOFy//gyenjtRs2ZprF/fXe50iEhmeXX+1vrSmKWlJaKiorLE79+/DwsLC50kRUTFlxACq1eHoF69Nbhw4TE2bLiEX3+9JndaRFREad1Zum/fvhgxYgQWLVoEd3d3KBQKnDhxApMmTeLt80T0QZ4+TcKIEbuxZ88NKebmZgsXl5IyZkVERZnWhdCiRYugUCgwePBgZGRkAAAMDQ3x+eef48cff9R5gkRUPBw4cAtDh+7CkydJUmzs2AZYuLA9TE0NZcyMiIqyXD9H6NWrV7h9+zaEEKhcuTJMTU11nVueYB8hooIlOTkdU6b8heXLz0mxUqVM4ef3Cbp0qSJjZkRUkMj+HKFXr15h0qRJ2LVrF9LT09G2bVssX74ctra2OkuGiIqXmJgkfPzxRly9GiPFPDxc4OfXjS9NJaJ8kePO0rNmzcL69evRuXNn9OvXD8HBwfj888/zMjciKuJsbU1RtuzrmyyMjQ3w88+dsHdvfxZBRJRvcnxpzNnZGXPnzkW/fv0AAOfOnUOzZs2QkpICfX39PE1Sl3hpjKhgefw4EYMH78KyZR1RrVopudMhogJK9ucIGRkZITIyEmXL/vsQMxMTE9y4cQPly5fXWUJ5jYUQkXx27boOa2tjtGpVUe5UiKiQkf05QiqVCkZGmk93NTAwkO4cIyJ6m6SkNIwatQc9emzDwIE7EReXLHdKREQAtOgsLYTA0KFDoVQqpVhKSgrGjBkDMzMzKbZz507dZkhEhVpIyCN4eu7EjRuxAICHDxOxfv1FeHs3lTkzIiItCqEhQ4ZkiQ0cOFCnyRBR0aFSqbFgwUnMnHkEGRlqAICpqSGWL++I4cPrypwdEdFrOS6E/P398zIPIipCoqLiMWhQEI4duyfFGjRwQEBAT1SpYiNjZkREmrR+sjQR0bts3XoVY8bsRXx8KgBAoQCmTWuOWbNawtCw8NxhSkTFAwshItKZ6OiXGDlyN5KS0gEAFSpYYfPmHmje3FHmzIiIsqf12+eJiN7G3t4cy5Z1BAD0718Dly6NYRFERAUaW4SIKNfS01VQqQSMjf/9Khk+vC4qVSqB1q2dZMyMiChn2CJERLly61Ycmjf3x8SJBzXiCoWCRRARFRq5KoQ2bdqEZs2awcHBAffuvb4rZOnSpfj99991mhwRFTxCCPj7h6FOndU4e/YhfHxCsHfvDbnTIiLKFa0LoVWrVsHb2xseHh548eIFVCoVAMDa2hpLly7VdX5EVIDExSWjT5/tGD783w7Rzs4lULq02XvmJCIqmLQuhFasWAFfX19Mnz5d42WrDRo0wJUrV3SaHBEVHIcPR6JWrVXYvj1cio0YURcXL45Bo0Zl3zEnEVHBpXVn6cjISNStm/WpsEqlEklJSTpJiogKjrQ0FWbMOIRFi04h8xXNJUoYw9e3K3r1qiZvckREH0jrQsjJyQkXL16Eo6PmLbH79+9HtWr8UiQqSmJiktCx42aEhUVLsY8/dsKGDd1Rtqzu3v5MRCQXrQuhSZMm4YsvvkBKSgqEEDh37hy2bNmC+fPnY+3atXmRIxHJxMbGBBYWr1+0bGioh/nzP4aXV1Po6SlkzoyISDe07iM0bNgwzJo1C5MnT8arV68wYMAArF69GsuWLUO/fv20TsDHxwdOTk4wNjZG/fr1cfz48RzNd/LkSRgYGKBOnTpar5OIckZfXw+bNvWAu3t5nDv3GSZOdGcRRERFikKIzKv+2nv27BnUajVKly6dq/m3bduGQYMGwcfHB82aNcOaNWuwdu1ahIeHo0KFCm+dLz4+HvXq1UPlypXx5MkTXLx4McfrTEhIgJWVFeLj42FpyaZ9ov/av/8mSpQwQZMm5TTiQggoFCyAiEg+eXX+/qBC6EM1btwY9erVw6pVq6SYm5sbunfvjvnz5791vn79+sHFxQX6+vrYtWsXCyGiD5ScnI5vvvkLK1acg5OTNS5eHANLS6XcaRERSfLq/J2rztLv+svwzp07OVpOWloaQkNDMWXKFI14+/btcerUqbfO5+/vj9u3b2Pz5s344Ycf3rue1NRUpKamSsMJCQk5yo+ouLh0KRqenjtx7dpTAEBk5AusW3cBXl5NZc6MiCjvaV0ITZgwQWM4PT0dYWFhOHDgACZNmpTj5Tx79gwqlQp2dnYacTs7O0RHR2c7z82bNzFlyhQcP34cBgY5S33+/PmYPXt2jvMiKi7UaoFly85gypS/kZb2+sGoxsYG+Omn9vj88wYyZ0dElD+0LoS++uqrbOMrV65ESEiI1gm82br0tr4IKpUKAwYMwOzZs1GlSpUcL3/q1Knw9vaWhhMSElC+fHmt8yQqSh49SsTQobsQHPxvC27t2nYIDOyFatVKyZgZEVH+0tlLVzt16oQdO3bkeHpbW1vo6+tnaf2JiYnJ0koEAImJiQgJCcG4ceNgYGAAAwMDfP/997h06RIMDAxw6NChbNejVCphaWmp8UNUnAUFRaBWrVUaRdDEiU1x9uxIFkFEVOxo3SL0Ntu3b0fJkiVzPL2RkRHq16+P4OBg9OjRQ4oHBwfjk08+yTK9paVllld4+Pj44NChQ9i+fTucnPi2a6L3efQoEf3770Bq6utLYQ4OFtiwoTvatq0kc2ZERPLQuhCqW7euxqUrIQSio6Px9OlT+Pj4aLUsb29vDBo0CA0aNEDTpk3xyy+/ICoqCmPGjAHw+rLWw4cPsXHjRujp6aFGjRoa85cuXRrGxsZZ4kSUPQcHCyxc2A7jxx9Ajx6u8PXtChsbU7nTIiKSjdaFUPfu3TWG9fT0UKpUKbRq1Qqurq5aLatv376IjY3F999/j8ePH6NGjRrYt2+f9PqOx48fIyoqStsUiej/qVRqqNUChob/viB53LhGqFSpBDw8XPhsICIq9rR6jlBGRgYCAgLQoUMH2Nvb52VeeYbPEaLiIioqHoMGBaFx47JYsKCd3OkQEX2QvDp/a9VZ2sDAAJ9//rnGc3mIqODZuvUqatVahWPH7mHhwlP4+++cPd+LiKi40fquscaNGyMsLCwvciGiD5SQkIrBg4PQv/8OxMe//oOlQgUrGBvr7L4IIqIiRetvx7Fjx2LixIl48OAB6tevDzMzM43xtWrV0llyRJRzJ09GYeDAINy9+0KKDRhQEytXesDa2li+xIiICrAc9xEaPnw4li5dCmtr66wLUSikByGqVCpd56hT7CNERU16ugpz5hzD3LnHoVa//jhbWirh4+MBT0/+YUJERYPsL13V19fH48ePkZyc/M7pMu/4KqhYCFFREhOThG7dtuDs2YdS7KOPKmDTph6oWNFavsSIiHRM9peuZtZLBb3QISpOSpQwRuafMvr6Csye3QpTpnwEfX2dPTSeiKhI0+rbks8cISpYDA31ERDQE3Xq2OPUqRGYPr0FiyAiIi3k+NKYnp4erKys3lsMxcXF6SSxvMJLY1SYHT4ciRIlTFCnjuZzvN72smIioqJC9ktjADB79mxYWVnpbOVElDNpaSrMmHEIixadQtWqtggNHQVTU0NpPIsgIqLc0aoQ6tevH0qXLp1XuRBRNq5ff4YBA3YgLCxaGvb1DcVXXzWROTMiosIvx50J+BcnUf4SQmD16hDUq7dGKoIMDfWwaFE7fPllY5mzIyIqGrS+a4yI8l5MTBJGjtyNPXtuSDE3N1sEBvbK0j+IiIhyL8eFkFqtzss8iOj/7d9/E8OG/Y4nT5Kk2NixDbBwYXuNfkFERPTh+AIiogLkwYMEfPLJVqSnv/7Do1QpU/j5fYIuXarInBkRUdHEB44QFSDlylni++9bAwA6daqMK1c+ZxFERJSH2CJEJCO1WkAIofEQxEmT3OHsXAK9e1fjTQpERHmMLUJEMnn0KBEdO27GnDnHNOL6+nr49NPqLIKIiPIBW4SIZBAUFIHPPtuD2Nhk/P13JNq3d4a7e3m50yIiKnZYCBHlo6SkNHh5HYSv7wUpZmdnhvR0lYxZEREVXyyEiPJJSMgjeHruxI0bsVKsRw9X+Pp2hY2NqYyZEREVXyyEiPKYSqXGggUnMXPmEWRkvL4t3tTUEMuXd8Tw4XXZF4iISEYshIjyUExMEj799DccO3ZPijVs6ICAgJ5wcbGRMTMiIgJ41xhRnrK0VOLFixQAgEIBTJ/eHCdPDmcRRERUQLAQIspDxsYGCAzsiapVbXD06FD88EMbGBrqy50WERH9P14aI9KhkyejUKKECapVKyXFqlcvjWvXxmo8NJGIiAoGfjMT6UB6ugozZx5GixbrMWDADqSmZmiMZxFERFQw8duZ6APdvh2H5s39MWfOMajVApcuPcEvv4TKnRYREeUAL40R5ZIQAhs2XMKXX+7Hy5dpAAB9fQVmz26FsWMbypscERHlCAsholyIi0vG6NF7sX17uBRzdi6BwMBeaNSorIyZERGRNlgIEWnp0KFIDB4chIcPE6XYiBF1sXRpR5ibG8mYGRERaYuFEJEWoqLi0aHDZukJ0SVKGMPXtyt69aomc2ZERJQb7CxNpIUKFawwdepHAIA2bZxw+fLnLIKIiAoxtggRvYMQAkIAenr/vg/s229bwNm5BAYNqq0RJyKiwoctQkRvEROThE8+2YqffjqlETc01MeQIXVYBBERFQFsESLKxv79NzFs2O948iQJBw7cwscfV0K9emXkTouIiHSMhRDRfyQnp+Obb/7CihXnpJi1tTGeP0+WMSsiIsorLISI/t+lS9Hw9NyJa9eeSrFOnSrD3/8T2NmZy5gZERHlFRZCVOyp1QLLlp3BlCl/Iy1NBeD1W+MXLmyHL75oCIWCfYGIiIoqFkJUrD19moQBA3bir7/uSLFatewQGNgT1auXljEzIiLKD7xrjIo1U1NDREXFS8MTJzbFuXMjWQQRERUTLISoWDMzM0JgYE9UrGiN4OBBWLSoPZRKNpQSERUX/ManYiUk5BFKlDCGs3NJKVa/vgNu3BgHQ0N9GTMjIiI5sEWIigWVSo3584+jadN18PTcifR0lcZ4FkFERMUTCyEq8qKi4tGmzUZMm3YIGRlqnD37EGvXXpA7LSIiKgB4aYyKtK1br2LMmL2Ij08FACgUwLRpzTFyZD2ZMyMiooKAhRAVSQkJqRg3bh82bbosxSpUsMLmzT3QvLmjjJkREVFBwkKIipxTp+5j4MCdiIx8IcUGDKiJlSs9YG1tLF9iRERU4LAQoiLl7t0XaNlyPTIy1AAAS0slfHw84OlZS+bMiIioIGJnaSpSKla0xpdfNgIANGtWHpcujWERREREb8UWISrUhBAAoPE+sHnzPkblyiUxalR9GBiw1iciorfjWYIKrbi4ZPTpsx0+Puc14sbGBhg7tiGLICIiei+2CFGhdPhwJAYNCsLDh4nYu/cGWrWqyPeDERGR1vgnMxUqaWkqTJ4cjI8/3oiHDxMBACYmBtL/iYiItMEWISo0IiKewtNzJ8LCoqVYmzZO2LChO8qVs5QxMyIiKqxYCFGBJ4TA6tUhmDjxTyQnZwAADA31MH/+x/Dyago9PcV7lkBERJQ9FkJUoMXGvsLQob9j794bUszNzRYBAT1Rt24ZGTMjIqKigH2EqEAzMNDDlStPpOGxYxsgJGQUiyAiItIJFkJUoFlZGWPz5p4oU8Yce/b0x8qVnWFqaih3WkREVETw0hgVKJcuRaNkSROUL28lxT76qALu3PkKxsb8dSUiIt2SvUXIx8cHTk5OMDY2Rv369XH8+PG3Trtz5060a9cOpUqVgqWlJZo2bYqDBw/mY7aUV9RqgSVLTqNRo7UYNCgIKpVaYzyLICIiyguyFkLbtm3DhAkTMH36dISFhaF58+bo1KkToqKisp3+2LFjaNeuHfbt24fQ0FC0bt0aXbt2RVhYWD5nTrr06FEiOnbcDG/vP5GWpsLRo/fg58djSkREeU8hMl/WJIPGjRujXr16WLVqlRRzc3ND9+7dMX/+/Bwto3r16ujbty9mzpyZo+kTEhJgZWWF+Ph4WFry2TNyCwqKwGef7UFsbLIUmzixKebObQOlkq1ARET0Wl6dv2U706SlpSE0NBRTpkzRiLdv3x6nTp3K0TLUajUSExNRsmTJt06TmpqK1NRUaTghISF3CZNOJSWlwcvrIHx9L0gxBwcLbNjQHW3bVpIxMyIiKk5kuzT27NkzqFQq2NnZacTt7OwQHR39lrk0/fTTT0hKSkKfPn3eOs38+fNhZWUl/ZQvX/6D8qYPFxLyCPXq/aJRBPXs6YbLl8ewCCIionwle2dphULzqcBCiCyx7GzZsgXfffcdtm3bhtKl3/6yzalTpyI+Pl76uX///gfnTLl3585zNG26DjduxAIAzMwMsW5dN2zf/ilsbExlzo6IiIob2QohW1tb6OvrZ2n9iYmJydJK9KZt27ZhxIgR+PXXX9G2bdt3TqtUKmFpaanxQ/KpVKkERoyoCwBo2NABYWGjMXx43RwVv0RERLomWyFkZGSE+vXrIzg4WCMeHBwMd3f3t863ZcsWDB06FIGBgejcuXNep0l54Kef2mPRonY4eXI4XFxs5E6HiIiKMVkvjXl7e2Pt2rXw8/NDREQEvLy8EBUVhTFjxgB4fVlr8ODB0vRbtmzB4MGD8dNPP6FJkyaIjo5GdHQ04uPj5doEeoeEhFQMHhwEf3/NW+HNzIwwcaI7DA31ZcqMiIjoNVnvT+7bty9iY2Px/fff4/Hjx6hRowb27dsHR0dHAMDjx481nim0Zs0aZGRk4IsvvsAXX3whxYcMGYL169fnd/r0DqdO3cfAgTsRGfkCQUHX0by5IypXfvvdfURERHKQ9TlCcuBzhPJWRoYac+YcxQ8/HIda/fpXy9JSiW3beqNjx8oyZ0dERIVVkXuOEBU9t2/HwdNzJ86efSjFPvqoAjZt6oGKFa3lS4yIiOgtWAjRBxNCYMOGS/jyy/14+TINAKCvr8Ds2a0wZcpH0NeX/SkNRERE2WIhRB/k+fNkjBq1F9u3h0sxZ+cSCAzshUaNysqYGRER0fuxEKIPolYLnDr170MqR4yoi6VLO8Lc3EjGrIiIiHKG1yzog9jYmGLDhu6wsTHB9u2fYu3abiyCiIio0GCLEGklIuIpSpY0gZ2duRRr27YSIiO/goWFUsbMiIiItMcWIcoRIQRWrw5B/fq/YNiw3/HmUxdYBBERUWHEQojeKyYmCZ98shWff/4HkpMzsH//LWzYcEnutIiIiD4YL43ROx04cAtDh+7CkydJUmzs2Abo06e6jFkRERHpBgshylZycjqmTPkLy5efk2KlSpnCz+8TdOlSRcbMiIiIdIeFEGVx5coTDBiwE1evxkgxDw8X+Pl10+gkTUREVNixECINt27FoUEDX6SlqQAAxsYGWLSoHcaObQiFQiFzdkRERLrFztKkoXLlkujb93X/n9q17RAaOgpffNGIRRARERVJbBGiLH7+2QMuLiUxeXIzKJX8FSEioqKLLULFWFJSGkaN2oNt265qxC0tlfj225YsgoiIqMjjma6YCgl5BE/PnbhxIxa//RYOd/fyKF/eSu60iIiI8hVbhIoZlUqN+fOPo2nTdbhxIxYAkJamwuXLT2TOjIiIKP+xRagYiYqKx6BBQTh27J4Ua9jQAQEBPeHiYiNjZkRERPJgIVRMbN16FWPG7EV8fCoAQKEApk1rjlmzWsLQUF/m7IiIiOTBQqiIS0hIxbhx+7Bp02UpVqGCFTZv7oHmzR1lzIyIiEh+LISKuFev0rF//y1puH//GvDx6Qxra2MZsyIiIioY2Fm6iLO3N8e6dd1gaanE5s09EBjYi0UQERHR/2OLUBFz61YcSpQwho2NqRTr1q0qIiO/QsmSJjJmRkREVPCwRaiIEELA3z8MdeqsxujReyGE0BjPIoiIiCgrFkJFQFxcMvr02Y7hw3cjKSkdO3ZEYMuWq++fkYiIqJjjpbFC7vDhSAwaFISHDxOl2IgRddGtW1UZsyIiIiocWAgVUmlpKsyYcQiLFp1C5lWwEiWM4evbFb16VZM3OSIiokKChVAhdP36MwwYsANhYdFSrE0bJ2zY0B3lylnKmBkREVHhwkKokPnnn2eoV28NkpMzAACGhnqYP/9jeHk1hZ6eQubsiIiIChd2li5kqlSxQadOLgAANzdbnDv3GSZOdGcRRERElAtsESpkFAoFfvmlC6pUKYlvv20JU1NDuVMiIiIqtBTizQfOFHEJCQmwsrJCfHw8LC0Ldn+a5OR0fPPNX2jXrhK6duVdYCQ/IQQyMjKgUqnkToWIiiBDQ0Po62f/IvC8On+zRaiAunQpGp6eO3Ht2lNs2XIVV658Dnt7c7nTomIsLS0Njx8/xqtXr+ROhYiKKIVCgXLlysHcPP/OdyyEChi1WmDZsjOYMuVvpKW9/qv75cs0hIQ8QpcuVWTOjoortVqNyMhI6Ovrw8HBAUZGRlAo2C+NiHRHCIGnT5/iwYMHcHFxeWvLkK6xECpAHj1KxNChuxAcfEeK1a5th8DAXqhWrZSMmVFxl5aWBrVajfLly8PU1PT9MxAR5UKpUqVw9+5dpKensxAqboKCIvDZZ3sQG5ssxSZObIq5c9tAqeRhooJBT483mhJR3pGjpZlnWJm9fJkGL68DWLs2TIo5OFhgw4buaNu2koyZERERFX38805mz58n47ffwqXhHj1ccfnyGBZBREQy+fbbbzFq1Ci50yhyUlNTUaFCBYSGhsqdigYWQjIrX94Ka9Z0gZmZIdau7YodO/rAxoZ9MIh07dSpU9DX10fHjh3lTiXP3b17FwqFQvqxsrJCkyZNsGfPnizTJicnY9asWahatSqUSiVsbW3Ru3dvXLt2Lcu0CQkJmD59OlxdXWFsbAx7e3u0bdsWO3fuRFF5EsuTJ0+wbNkyTJs2Te5U8swvv/yCVq1awdLSEgqFAi9evMjRfD4+PnBycoKxsTHq16+P48ePa4wXQuC7776Dg4MDTExM0KpVK43fI6VSia+//hrffPONLjfng7EQymdRUfFISEjViPXtWwO3bo3HiBH1eCcOUR7x8/PDl19+iRMnTiAqKipP16VSqaBWq/N0HTnx119/4fHjxzh79iwaNWqEXr164erVq9L41NRUtG3bFn5+fpgzZw5u3LiBffv2QaVSoXHjxjhz5ow07YsXL+Du7o6NGzdi6tSpuHDhAo4dO4a+ffti8uTJiI+Pz7ftSk9Pz7Nlr1u3Dk2bNkXFihU/aDl5meOHevXqFTp27KhVsbdt2zZMmDAB06dPR1hYGJo3b45OnTppfJYWLFiAxYsX4+eff8b58+dhb2+Pdu3aITExUZrG09MTx48fR0REhE636YOIYiY+Pl4AEPHx8fm+7i1brggrq/li8OCgfF830YdITk4W4eHhIjk5We5UcuXly5fCwsJCXL9+XfTt21fMnj1bGtekSRPxzTffaEwfExMjDAwMxKFDh4QQQqSmpopJkyYJBwcHYWpqKho1aiQOHz4sTe/v7y+srKzEnj17hJubm9DX1xd37twR586dE23bthU2NjbC0tJStGjRQoSGhmqsKyIiQjRr1kwolUrh5uYmgoODBQARFBQkTfPgwQPRp08fYW1tLUqWLCm6desmIiMj37q9kZGRAoAICwuTYgkJCQKAWL58uRT78ccfhUKhEBcvXtSYX6VSiQYNGohq1aoJtVothBDi888/F2ZmZuLhw4dZ1peYmCjS09Pfms/vv/8u6tevL5RKpbCxsRE9evSQxr25rUIIYWVlJfz9/TW2Zdu2baJly5ZCqVSKpUuXCmNjY7F//36N+Xbs2CFMTU1FYmJirvabEELUrFlT/Pzzzxqx/fv3i2bNmgkrKytRsmRJ0blzZ3Hr1i1pfHY5+vn5CSGE8PPzE66urkKpVIqqVauKlStXaix78uTJwsXFRZiYmAgnJycxY8YMkZaW9s4cdeXw4cMCgHj+/Pl7p23UqJEYM2aMRszV1VVMmTJFCCGEWq0W9vb24scff5TGp6SkCCsrK7F69WqN+Vq1aiW+/fbbbNfzru+avDp/s0UoHyQkpGLw4CD0778D8fGp2LjxEnbsCH//jESkE9u2bUPVqlVRtWpVDBw4EP7+/tKlHE9PT2zZskXj0s62bdtgZ2eHli1bAgCGDRuGkydPYuvWrbh8+TI+/fRTdOzYETdv3pTmefXqFebPn4+1a9fi2rVrKF26NBITEzFkyBAcP34cZ86cgYuLCzw8PKS/kNVqNbp37w5TU1OcPXsWv/zyC6ZPn66R+6tXr9C6dWuYm5vj2LFjOHHiBMzNzdGxY0ekpaXlaPvT09Ph6+sL4PWTezMFBgaiXbt2qF27tsb0enp68PLyQnh4OC5dugS1Wo2tW7fC09MTDg4OWZZvbm4OA4Ps7735448/0LNnT3Tu3BlhYWH4+++/0aBBgxzl/V/ffPMNxo8fj4iICHz66afo3LkzAgICNKYJDAzEJ598AnNz81ztt+fPn+Pq1atZ8ktKSoK3tzfOnz+Pv//+G3p6eujRo0eWVr//5tihQwf4+vpi+vTpmDt3LiIiIjBv3jx8++232LBhgzSPhYUF1q9fj/DwcCxbtgy+vr5YsmTJO/dF9erVYW5u/taf6tWra7Nr3ystLQ2hoaFo3769Rrx9+/Y4deoUACAyMhLR0dEa0yiVSrRs2VKaJlOjRo2yXFaTE+8ay2MnT0Zh4MAg3L37Qor1718DH3/MztBUBGxuACRF5/96zeyBgSE5nnzdunUYOHAgAKBjx454+fIl/v77b7Rt2xZ9+/aFl5cXTpw4gebNmwN4fUIdMGAA9PT0cPv2bWzZsgUPHjyQioCvv/4aBw4cgL+/P+bNmwfgdbHh4+OjUVS0adNGI481a9agRIkSOHr0KLp06YI///wTt2/fxpEjR2Bvbw8AmDt3Ltq1ayfNs3XrVujp6WHt2rXSpXN/f39YW1vjyJEjWU5O/+Xu7g49PT0kJydDrVajYsWK6NOnjzT+xo0baN26dbbzurm5SdM4ODjg+fPncHV1zcHe1jR37lz069cPs2fPlmJvFl45MWHCBPTs2VMa9vT0xODBg/Hq1SuYmpoiISEBf/zxB3bs2AEgd/vt3r17EEJkKfZ69eqlMbxu3TqULl0a4eHhqFGjxltznDNnDn766Scp5uTkhPDwcKxZswZDhgwBAMyYMUOavmLFipg4cSK2bduGyZMnv3Vf7Nu3752X3v5b7OrCs2fPoFKpYGdnpxG3s7NDdPTrz3/mv9lNc+/ePY1Y2bJlcffuXZ3m+CFYCOWR9HQV5sw5hrlzj0Otfv2XpqWlEj4+HvD0rCVzdkQ6khQNvHwodxbv9M8//+DcuXPYuXMnAMDAwAB9+/aFn58f2rZti1KlSqFdu3YICAhA8+bNERkZidOnT2PVqlUAgAsXLkAIgSpVNJ/snpqaChsbG2nYyMgItWppfrZjYmIwc+ZMHDp0CE+ePIFKpcKrV6+kfhX//PMPypcvLxVBwOu/lv8rNDQUt27dgoWFhUY8JSUFt2/ffue2b9u2Da6urrhx4wYmTJiA1atXo2TJkjnZbVILmUKh0Pi/ti5evIjPPvtM6/ne9GYrTefOnWFgYIDdu3ejX79+2LFjBywsLKQCJzf7LTn59XPcjI2NNeK3b9/Gt99+izNnzuDZs2dSS1BUVJRGIfTfHJ8+fYr79+9jxIgRGtufkZEBKysraXj79u1YunQpbt26hZcvXyIjI+O979FydHR85/i88ubxF0JkieVkGhMTkwL1qh4WQnng1q04DBy4E2fP/nuCaNasPDZv7omKFa3lS4xI18zs3z+NzOtdt24dMjIyULZsWSkmhIChoSGeP3+OEiVKwNPTE1999RVWrFiBwMBAVK9eXWq1UKvV0NfXR2hoaJYn3f73fUgmJiZZvvCHDh2Kp0+fYunSpXB0dIRSqUTTpk2lSzPZnSTepFarUb9+/SyXgYDXT+F9l/Lly8PFxQUuLi4wNzdHr169EB4ejtKlSwMAqlSpgvDw7C/TX79+HQDg4uKCUqVKoUSJErnq4GpiYvLO8f8ttDJl19phZmamMWxkZITevXsjMDAQ/fr1Q2BgIPr27StdosvNfrO1tQXw+hLZf6fp2rUrypcvD19fXzg4OECtVqNGjRpZLrH9N8fMYsnX1xeNGzfWmC7z9+jMmTNSa1mHDh1gZWWFrVu34qeffso2v0zVq1fP0sryX46Ojtne9Zdbtra20NfXl1p9MsXExEgtQJnFfHR0NMqUKZPtNJni4uLe+7ubn1gI6VhExFM0bOiLpKTXH2R9fQW++64Vpkz5CAYG7JJFRYwWl6fkkJGRgY0bN+Knn37KcimkV69eCAgIwLhx49C9e3eMHj0aBw4cQGBgIAYNGiRNV7duXahUKsTExEiXznLq+PHj8PHxgYeHBwDg/v37ePbsmTTe1dUVUVFRePLkiXSyOH/+vMYy6tWrh23btqF06dIf9Mbtli1bokaNGpg7dy6WLVsGAOjXrx+mT5+OS5cuaVyuUqvVWLJkCapVq4batWtDoVCgb9++2LRpE2bNmpXl0lFSUhKUSmW2/YRq1aqFv//+G8OGDcs2r1KlSuHx48fS8M2bN3PcWuDp6Yn27dvj2rVrOHz4MObMmSONy81+c3Z2hqWlJcLDw6UWwNjYWERERGDNmjXS8T9x4sR7l2VnZ4eyZcvizp078PT0zHaakydPwtHRUaNf2LsKnEz5fWnMyMgI9evXR3BwMHr06CHFg4OD8cknnwB4fdnP3t4ewcHBqFu3LoDXfYuOHj2K//3vfxrLu3r1qjRNgaDTrteFQF7fNaZWq0XHjpsF8J1wdl4mzpy5nyfrIcpPhfWusaCgIGFkZCRevHiRZdy0adNEnTp1pOEBAwaI2rVrC4VCIe7du6cxraenp6hYsaLYsWOHdDfYjz/+KP744w8hxL93jb2pTp06ol27diI8PFycOXNGNG/eXJiYmIglS5YIIYTIyMgQVatWFR06dBCXLl0SJ06cEI0bNxYAxK5du4QQQiQlJQkXFxfRqlUrcezYMXHnzh1x5MgRMX78eHH/fvbfL9ndNSaEELt37xZKpVI8ePBACPH6uDZu3FiUL19e/Prrr+LevXvi3Llzonv37sLMzEycPn1amjcuLk64urqKcuXKiQ0bNohr166JGzduiHXr1onKlSu/9c6jw4cPCz09PTFz5kwRHh4uLl++LP73v/9J4/v16yfc3NxEaGioOH/+vGjTpo0wNDTMctfYm9sixOvv23LlyonatWsLZ2dnjXG52W9CCNGzZ08xceJEaVilUgkbGxsxcOBAcfPmTfH333+Lhg0batzt9rYcfX19hYmJiVi6dKn4559/xOXLl4Wfn5/46aefhBBC7Nq1SxgYGIgtW7aIW7duiWXLlomSJUtm+7ukS48fPxZhYWHC19dXABDHjh0TYWFhIjY2VpqmTZs2YsWKFdLw1q1bhaGhoVi3bp0IDw8XEyZMEGZmZuLu3bvSND/++KOwsrISO3fuFFeuXBH9+/cXZcqUEQkJCRrrd3R0FBs3bsw2NznuGmMhlAceP04UX321XyQmpubZOojyU2EthLp06SI8PDyyHRcaGioASLez//HHHwKAaNGiRZZp09LSxMyZM0XFihWFoaGhsLe3Fz169BCXL18WQry9ELpw4YJo0KCBUCqVwsXFRfz222/C0dFRKoSE+Pf2eSMjI+Hq6ir27NkjAIgDBw5I0zx+/FgMHjxY2NraCqVSKSpVqiQ+++yzt36Pve3ErFarRdWqVcXnn38uxZKSksSMGTNE5cqVhaGhoShZsqTo1auXuHLlSpblvnjxQkyZMkW4uLgIIyMjYWdnJ9q2bSuCgoKk2+yzs2PHDlGnTh1hZGQkbG1tRc+ePaVxDx8+FO3btxdmZmbCxcVF7Nu3L9vb57MrhIQQYtKkSQKAmDlzZpZx2u43IYQ4cOCAKFu2rFCpVFIsODhYuLm5CaVSKWrVqiWOHDmSo0JICCECAgKkbS9RooRo0aKF2Llzp0b+NjY2wtzcXPTt21csWbIkzwuhWbNmCQBZfjL3uRCvi5VZs2ZpzLdy5Urh6OgojIyMRL169cTRo0c1xqvVajFr1ixhb28vlEqlaNGiRZbfo1OnTglra2vx6tWrbHOToxBSCFFEHgeaQwkJCbCyskJ8fPwHNTMDQFqaCt9+ewjt2jnzlRhUpKWkpCAyMlJ6qizlnZMnT+Kjjz7CrVu34OzsLHc6xY4QAk2aNMGECRPQv39/udMpcj799FPUrVv3rQ9zfNd3jS7P3//FPkK5dP36MwwYsANhYdHYvPkKLl8ew1djEJHWgoKCYG5uDhcXF9y6dQtfffUVmjVrxiJIJgqFAr/88gsuX74sdypFTmpqKmrXrg0vLy+5U9HAQkhLQgisWRMKb++DSE7OAAA8fZqEU6fuo2vXqjJnR0SFTWJiIiZPnoz79+/D1tYWbdu2fe9dQ5S3ateunatnHdG7KZVKjecmFRQshLQQE5OEkSN3Y8+eG1LMzc0WgYG9UKeOTLcRE1GhNnjwYAwePFjuNIiKLRZCOXTgwC0MHboLT54kSbGxYxtg4cL2MDXV7a2KRERElD9YCL1HcnI6pkz5C8uXn5NipUqZws/vE3TpUuUdcxIREVFBx0LoPR49SsS6dWHSsIeHC/z8usHOzvwdcxEVTcXsJlMiymdyfMfwUcfv4excEsuXd4KxsQF+/rkT9u7tzyKIip3MJ9UWpPcDEVHRk/nakjdfZ5OX2CL0hkePEmFtbazR72fYsDr4+GMnODpay5cYkYz09fVhbW2NmJgYAICpqWmuXsBJRPQ2arUaT58+hampabava8krLIT+IygoAp99tgeffloNq1Z1keIKhYJFEBV7mS9VzCyGiIh0TU9PDxUqVMjXP7RYCAF4+TINXl4HsHbt675Aq1eHonPnKuwMTfQfCoUCZcqUQenSpd/5wkciotwyMjKCnl7+9tqRvRDy8fHBwoUL8fjxY1SvXh1Lly595xuejx49Cm9vb1y7dg0ODg6YPHkyxowZk+v1nz//EJ6eO3HzZpwU69HDFU2blsv1MomKMn19/Xy9fk9ElJdk7Sy9bds2TJgwAdOnT0dYWBiaN2+OTp06ISoqKtvpIyMj4eHhgebNmyMsLAzTpk3D+PHjsWPHDq3XrVKpMX/+cbi7+0lFkKmpIdau7YodO/rwdRlERETFgKwvXW3cuDHq1auHVatWSTE3Nzd0794d8+fPzzL9N998g927dyMiIkKKjRkzBpcuXcLp06dztM7Ml7a5u/vg1Kl/+zo0bOiAgICecHGx+YAtIiIioryQVy9dla1FKC0tDaGhoWjfvr1GvH379jh16lS285w+fTrL9B06dEBISIjWfRZOnXrd6qSnp8D06c1x8uRwFkFERETFjGx9hJ49ewaVSgU7OzuNuJ2dHaKjo7OdJzo6OtvpMzIy8OzZM5QpUybLPKmpqUhNTZWG4+PjM8egXDkr+Pp2gbt7BSQnJyE5+cO2iYiIiPJGQkICAN0/dFH2ztJv3iInhHjnbXPZTZ9dPNP8+fMxe/bsbMYswYMHQKdOU7VLmIiIiGQTGxsLKysrnS1PtkLI1tYW+vr6WVp/YmJisrT6ZLK3t892egMDA9jYZH9Za+rUqfD29paGX7x4AUdHR0RFRel0R1LuJCQkoHz58rh//75Or/mS9ngsCg4ei4KDx6LgiI+PR4UKFVCyZEmdLle2QsjIyAj169dHcHAwevToIcWDg4PxySefZDtP06ZNsWfPHo3Yn3/+iQYNGkivAHiTUqmEUqnMEreysuIvdQFiaWnJ41FA8FgUHDwWBQePRcGh6+cMyXr7vLe3N9auXQs/Pz9ERETAy8sLUVFR0nOBpk6disGDB0vTjxkzBvfu3YO3tzciIiLg5+eHdevW4euvv5ZrE4iIiKgQk7WPUN++fREbG4vvv/8ejx8/Ro0aNbBv3z44OjoCAB4/fqzxTCEnJyfs27cPXl5eWLlyJRwcHLB8+XL06tVLrk0gIiKiQkz2ztJjx47F2LFjsx23fv36LLGWLVviwoULuV6fUqnErFmzsr1cRvmPx6Pg4LEoOHgsCg4ei4Ijr46FrA9UJCIiIpKTrH2EiIiIiOTEQoiIiIiKLRZCREREVGyxECIiIqJiq0gWQj4+PnBycoKxsTHq16+P48ePv3P6o0ePon79+jA2NkalSpWwevXqfMq06NPmWOzcuRPt2rVDqVKlYGlpiaZNm+LgwYP5mG3Rp+1nI9PJkydhYGCAOnXq5G2CxYi2xyI1NRXTp0+Ho6MjlEolnJ2d4efnl0/ZFm3aHouAgADUrl0bpqamKFOmDIYNG4bY2Nh8yrboOnbsGLp27QoHBwcoFArs2rXrvfPo5PwtipitW7cKQ0ND4evrK8LDw8VXX30lzMzMxL1797Kd/s6dO8LU1FR89dVXIjw8XPj6+gpDQ0Oxffv2fM686NH2WHz11Vfif//7nzh37py4ceOGmDp1qjA0NBQXLlzI58yLJm2PR6YXL16ISpUqifbt24vatWvnT7JFXG6ORbdu3UTjxo1FcHCwiIyMFGfPnhUnT57Mx6yLJm2PxfHjx4Wenp5YtmyZuHPnjjh+/LioXr266N69ez5nXvTs27dPTJ8+XezYsUMAEEFBQe+cXlfn7yJXCDVq1EiMGTNGI+bq6iqmTJmS7fSTJ08Wrq6uGrHRo0eLJk2a5FmOxYW2xyI71apVE7Nnz9Z1asVSbo9H3759xYwZM8SsWbNYCOmItsdi//79wsrKSsTGxuZHesWKtsdi4cKFolKlShqx5cuXi3LlyuVZjsVRTgohXZ2/i9SlsbS0NISGhqJ9+/Ya8fbt2+PUqVPZznP69Oks03fo0AEhISFIT0/Ps1yLutwcizep1WokJibq/AV7xVFuj4e/vz9u376NWbNm5XWKxUZujsXu3bvRoEEDLFiwAGXLlkWVKlXw9ddfIzk5OT9SLrJycyzc3d3x4MED7Nu3D0IIPHnyBNu3b0fnzp3zI2X6D12dv2V/srQuPXv2DCqVKsvb6+3s7LK8tT5TdHR0ttNnZGTg2bNnKFOmTJ7lW5Tl5li86aeffkJSUhL69OmTFykWK7k5Hjdv3sSUKVNw/PhxGBgUqa8KWeXmWNy5cwcnTpyAsbExgoKC8OzZM4wdOxZxcXHsJ/QBcnMs3N3dERAQgL59+yIlJQUZGRno1q0bVqxYkR8p03/o6vxdpFqEMikUCo1hIUSW2Pumzy5O2tP2WGTasmULvvvuO2zbtg2lS5fOq/SKnZweD5VKhQEDBmD27NmoUqVKfqVXrGjz2VCr1VAoFAgICECjRo3g4eGBxYsXY/369WwV0gFtjkV4eDjGjx+PmTNnIjQ0FAcOHEBkZKT0snDKX7o4fxepP/NsbW2hr6+fpZKPiYnJUjVmsre3z3Z6AwMD2NjY5FmuRV1ujkWmbdu2YcSIEfjtt9/Qtm3bvEyz2ND2eCQmJiIkJARhYWEYN24cgNcnYyEEDAwM8Oeff6JNmzb5kntRk5vPRpkyZVC2bFlYWVlJMTc3Nwgh8ODBA7i4uORpzkVVbo7F/Pnz0axZM0yaNAkAUKtWLZiZmaF58+b44YcfeBUhH+nq/F2kWoSMjIxQv359BAcHa8SDg4Ph7u6e7TxNmzbNMv2ff/6JBg0awNDQMM9yLepycyyA1y1BQ4cORWBgIK+565C2x8PS0hJXrlzBxYsXpZ8xY8agatWquHjxIho3bpxfqRc5uflsNGvWDI8ePcLLly+l2I0bN6Cnp4dy5crlab5FWW6OxatXr6Cnp3nq1NfXB/BvawTlD52dv7XqWl0IZN4KuW7dOhEeHi4mTJggzMzMxN27d4UQQkyZMkUMGjRImj7z9jsvLy8RHh4u1q1bx9vndUTbYxEYGCgMDAzEypUrxePHj6WfFy9eyLUJRYq2x+NNvGtMd7Q9FomJiaJcuXKid+/e4tq1a+Lo0aPCxcVFjBw5Uq5NKDK0PRb+/v7CwMBA+Pj4iNu3b4sTJ06IBg0aiEaNGsm1CUVGYmKiCAsLE2FhYQKAWLx4sQgLC5MeZZBX5+8iVwgJIcTKlSuFo6OjMDIyEvXq1RNHjx6Vxg0ZMkS0bNlSY/ojR46IunXrCiMjI1GxYkWxatWqfM646NLmWLRs2VIAyPIzZMiQ/E+8iNL2s/FfLIR0S9tjERERIdq2bStMTExEuXLlhLe3t3j16lU+Z100aXssli9fLqpVqyZMTExEmTJlhKenp3jw4EE+Z130HD58+J3ngLw6fyuEYFseERERFU9Fqo8QERERkTZYCBEREVGxxUKIiIiIii0WQkRERFRssRAiIiKiYouFEBERERVbLISIiIio2GIhREQa1q9fD2tra7nTyLWKFSti6dKl75zmu+++Q506dfIlHyIq2FgIERVBQ4cOhUKhyPJz69YtuVPD+vXrNXIqU6YM+vTpg8jISJ0s//z58xg1apQ0rFAosGvXLo1pvv76a/z99986Wd/bvLmddnZ26Nq1K65du6b1cgpzYUpU0LEQIiqiOnbsiMePH2v8ODk5yZ0WgNcvdX38+DEePXqEwMBAXLx4Ed26dYNKpfrgZZcqVQqmpqbvnMbc3Fyrt1Pn1n+3848//kBSUhI6d+6MtLS0PF83EeUMCyGiIkqpVMLe3l7jR19fH4sXL0bNmjVhZmaG8uXLY+zYsRpvNX/TpUuX0Lp1a1hYWMDS0hL169dHSEiINP7UqVNo0aIFTExMUL58eYwfPx5JSUnvzE2hUMDe3h5lypRB69atMWvWLFy9elVqsVq1ahWcnZ1hZGSEqlWrYtOmTRrzf/fdd6hQoQKUSiUcHBwwfvx4adx/L41VrFgRANCjRw8oFApp+L+Xxg4ePAhjY2O8ePFCYx3jx49Hy5YtdbadDRo0gJeXF+7du4d//vlHmuZdx+PIkSMYNmwY4uPjpZal7777DgCQlpaGyZMno2zZsjAzM0Pjxo1x5MiRd+ZDRFmxECIqZvT09LB8+XJcvXoVGzZswKFDhzB58uS3Tu/p6Yly5crh/PnzCA0NxZQpU2BoaAgAuHLlCjp06ICePXvi8uXL2LZtG06cOIFx48ZplZOJiQkAID09HUFBQfjqq68wceJEXL16FaNHj8awYcNw+PBhAMD27duxZMkSrFmzBjdv3sSuXbtQs2bNbJd7/vx5AIC/vz8eP34sDf9X27ZtYW1tjR07dkgxlUqFX3/9FZ6enjrbzhcvXiAwMBAApP0HvPt4uLu7Y+nSpVLL0uPHj/H1118DAIYNG4aTJ09i69atuHz5Mj799FN07NgRN2/ezHFORAQUybfPExV3Q4YMEfr6+sLMzEz66d27d7bT/vrrr8LGxkYa9vf3F1ZWVtKwhYWFWL9+fbbzDho0SIwaNUojdvz4caGnpyeSk5OznefN5d+/f180adJElCtXTqSmpgp3d3fx2Wefaczz6aefCg8PDyGEED/99JOoUqWKSEtLy3b5jo6OYsmSJdIwABEUFKQxzaxZs0Tt2rWl4fHjx4s2bdpIwwcPHhRGRkYiLi7ug7YTgDAzMxOmpqbSm7S7deuW7fSZ3nc8hBDi1q1bQqFQiIcPH2rEP/74YzF16tR3Lp+INBnIW4YRUV5p3bo1Vq1aJQ2bmZkBAA4fPox58+YhPDwcCQkJyMjIQEpKCpKSkqRp/svb2xsjR47Epk2b0LZtW3z66adwdnYGAISGhuLWrVsICAiQphdCQK1WIzIyEm5ubtnmFh8fD3Nzcwgh8OrVK9SrVw87d+6EkZERIiIiNDo7A0CzZs2wbNkyAMCnn36KpUuXolKlSujYsSM8PDzQtWtXGBjk/uvM09MTTZs2xaNHj+Dg4ICAgAB4eHigRIkSH7SdFhYWuHDhAjIyMnD06FEsXLgQq1ev1phG2+MBABcuXIAQAlWqVNGIp6am5kvfJ6KihIUQURFlZmaGypUra8Tu3bsHDw8PjBkzBnPmzEHJkiVx4sQJjBgxAunp6dku57vvvsOAAQPwxx9/YP/+/Zg1axa2bt2KHj16QK1WY/To0Rp9dDJVqFDhrbllFgh6enqws7PLcsJXKBQaw0IIKVa+fHn8888/CA4Oxl9//YWxY8di4cKFOHr0qMYlJ200atQIzs7O2Lp1Kz7//HMEBQXB399fGp/b7dTT05OOgaurK6Kjo9G3b18cO3YMQO6OR2Y++vr6CA0Nhb6+vsY4c3NzrbadqLhjIURUjISEhCAjIwM//fQT9PRedxH89ddf3ztflSpVUKVKFXh5eaF///7w9/dHjx49UK9ePVy7di1LwfU+/y0Q3uTm5oYTJ05g8ODBUuzUqVMarS4mJibo1q0bunXrhi+++AKurq64cuUK6tWrl2V5hoaGObobbcCAAQgICEC5cuWgp6eHzp07S+Nyu51v8vLywuLFixEUFIQePXrk6HgYGRllyb9u3bpQqVSIiYlB8+bNPygnouKOnaWJihFnZ2dkZGRgxYoVuHPnDjZt2pTlUs1/JScnY9y4cThy5Aju3buHkydP4vz581JR8s033+D06dP44osvcPHiRdy8eRO7d+/Gl19+mescJ02ahPXr12P16tW4efMmFi9ejJ07d0qdhNevX49169bh6tWr0jaYmJjA0dEx2+VVrFgRf//9N6Kjo/H8+fO3rtfT0xMXLlzA3Llz0bt3bxgbG0vjdLWdlpaWGDlyJGbNmgUhRI6OR8WKFfHy5Uv8/fffePbsGV69eoUqVarA09MTgwcPxs6dOxEZGYnz58/jf//7H/bt26dVTkTFnpwdlIgobwwZMkR88skn2Y5bvHixKFOmjDAxMREdOnQQGzduFADE8+fPhRCanXNTU1NFv379RPny5YWRkZFwcHAQ48aN0+ggfO7cOdGuXTthbm4uzMzMRK1atcTcuXPfmlt2nX/f5OPjIypVqiQMDQ1FlSpVxMaNG6VxQUFBonHjxsLS0lKYmZmJJk2aiL/++ksa/2Zn6d27d4vKlSsLAwMD4ejoKITI2lk6U8OGDQUAcejQoSzjdLWd9+7dEwYGBmLbtm1CiPcfDyGEGDNmjLCxsREAxKxZs4QQQqSlpYmZM2eKihUrCkNDQ2Fvby969OghLl++/NaciCgrhRBCyFuKEREREcmDl8aIiIio2GIhRERERMUWCyEiIiIqtlgIERERUbHFQoiIiIiKLRZCREREVGyxECIiIqJii4UQERERFVsshIiIiKjYYiFERERExRYLISIiIiq2WAgRERFRsfV/rtG/8dEjG0MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# # Compute ROC curve and ROC area\n",
    "# fpr, tpr, _ = roc_curve(y_test, y_pred_class)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # Plot\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')  # random classifier\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "# plt.savefig('./output/lightGBM.png')\n",
    "# plt.show()\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "n_classes = y_test_bin.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_test_pred_bin[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot average ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='darkorange', lw=2, label='Average ROC curve (area = %0.2f)' % roc_auc[\"macro\"])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('./output/lightGBM.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# # LightGBM\n",
    "# lgb_auprc = average_precision_score(y_test, y_pred_class)\n",
    "# print(f'LightGBM AUPRC: {lgb_auprc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49fe29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
